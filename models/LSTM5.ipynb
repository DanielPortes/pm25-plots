{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:54:05.717610Z",
     "start_time": "2024-08-14T13:40:05.658016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from copy import deepcopy as dc\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "data_hoje = datetime.now().strftime('%d-%m')\n",
    "inicio_execucao = pd.Timestamp.now()\n",
    "\n",
    "os.makedirs(f'../logs/{data_hoje}', exist_ok=True)\n",
    "os.makedirs(f'../plots/{data_hoje}', exist_ok=True)\n",
    "os.makedirs(f'../best_models/{data_hoje}', exist_ok=True)\n",
    "\n",
    "logging.basicConfig(filename=f'../logs/{data_hoje}/bilstm_optuna.log', level=logging.INFO, format='- %(message)s')\n",
    "logging.info('-' * 50)\n",
    "logging.info(f'{inicio_execucao} - Iniciando o processo de otimização e treinamento do modelo BiLSTM')\n",
    "\n",
    "df_original = pd.read_csv('../dados_tratados/combinado/Piratininga/Piratininga_tratado_combinado.csv',\n",
    "                          usecols=['PM2.5', 'Data e Hora', 'PM10', 'Monóxido de Carbono'], low_memory=False)\n",
    "\n",
    "df_original['Data e Hora'] = pd.to_datetime(df_original['Data e Hora'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_original.index = df_original['Data e Hora']\n",
    "df_original.sort_index(inplace=True)\n",
    "\n",
    "colunas_selecionadas = ['PM2.5', 'PM10', 'Monóxido de Carbono']\n",
    "df = df_original[colunas_selecionadas]\n",
    "df = df.loc['2019-01-01':'2022-01-01']\n",
    "\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "def impute_missing_data(df):\n",
    "    random_imputer = SimpleImputer(strategy='mean')\n",
    "    df_random_imputed = pd.DataFrame(random_imputer.fit_transform(df), columns=df.columns, index=df.index)\n",
    "    df_interpolated = df_random_imputed.interpolate(method='time')\n",
    "    knn_imputer = KNNImputer(n_neighbors=5)\n",
    "    df_knn_imputed = pd.DataFrame(knn_imputer.fit_transform(df_interpolated), columns=df.columns, index=df.index)\n",
    "    return df_interpolated\n",
    "\n",
    "df_imputed = impute_missing_data(df)\n",
    "\n",
    "logging.info(f\"Dados ausentes antes da imputação: {df.isna().sum()}\")\n",
    "logging.info(f\"Dados ausentes após a imputação: {df_imputed.isna().sum()}\")\n",
    "\n",
    "def add_cyclical_features(df):\n",
    "    df['hour'] = df.index.hour\n",
    "    df['day'] = df.index.day\n",
    "    df['month'] = df.index.month\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    \n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    \n",
    "    df.drop(['hour', 'day', 'month', 'day_of_week'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "df_imputed = add_cyclical_features(df_imputed)\n",
    "\n",
    "def prepare_dataframe_for_lstm(df, n_steps):\n",
    "    df = dc(df)\n",
    "    for col in df.columns:\n",
    "        for i in range(1, n_steps + 1):\n",
    "            df[f'{col}(t-{i})'] = df[col].shift(i)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "lookback = 8\n",
    "shifted_df = prepare_dataframe_for_lstm(df_imputed, lookback)\n",
    "\n",
    "preprocessing_scaler = StandardScaler()\n",
    "shifted_df_as_np = preprocessing_scaler.fit_transform(shifted_df)\n",
    "\n",
    "X = shifted_df_as_np[:, len(colunas_selecionadas):]\n",
    "y = shifted_df_as_np[:, 0]\n",
    "\n",
    "X = dc(np.flip(X, axis=1))\n",
    "\n",
    "train_split = int(len(X) * 0.7)\n",
    "val_split = int(len(X) * 0.85)\n",
    "\n",
    "X_train, X_val, X_test = X[:train_split], X[train_split:val_split], X[val_split:]\n",
    "y_train, y_val, y_test = y[:train_split], y[train_split:val_split], y[val_split:]\n",
    "\n",
    "X_train = X_train.reshape((-1, lookback, X_train.shape[1] // lookback))\n",
    "X_val = X_val.reshape((-1, lookback, X_val.shape[1] // lookback))\n",
    "X_test = X_test.reshape((-1, lookback, X_test.shape[1] // lookback))\n",
    "y_train = y_train.reshape((-1, 1))\n",
    "y_val = y_val.reshape((-1, 1))\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_val = torch.tensor(X_val).float()\n",
    "y_val = torch.tensor(y_val).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_size = trial.suggest_int('hidden_size', 32, 512)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 5)\n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.7)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [128])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True)\n",
    "\n",
    "    model = BiLSTM(input_size=X_train.shape[2], hidden_size=hidden_size, \n",
    "                   num_layers=num_layers, dropout=dropout).to(device)\n",
    "\n",
    "    criterion = nn.HuberLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    train_loader = DataLoader(TimeSeriesDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TimeSeriesDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        intermediate_value = val_loss\n",
    "        trial.report(intermediate_value, epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('Value: ', trial.value)\n",
    "print('Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "\n",
    "best_params = study.best_params\n",
    "final_model = BiLSTM(input_size=X_train.shape[2], \n",
    "                     hidden_size=best_params['hidden_size'],\n",
    "                     num_layers=best_params['num_layers'], \n",
    "                     dropout=best_params['dropout']).to(device)\n",
    "\n",
    "criterion = nn.HuberLoss()\n",
    "optimizer = optim.AdamW(final_model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])\n",
    "\n",
    "scaler = GradScaler()\n",
    "num_workers = 0\n",
    "\n",
    "train_loader = DataLoader(TimeSeriesDataset(X_train, y_train), batch_size=best_params['batch_size'], shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(TimeSeriesDataset(X_val, y_val), batch_size=best_params['batch_size'], num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "num_epochs = 500\n",
    "best_val_loss = float('inf')\n",
    "patience = 20\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    final_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Usar precisão mista para forward pass\n",
    "        with autocast():\n",
    "            outputs = final_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Escalar a perda e fazer o backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    final_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            with autocast():\n",
    "                outputs = final_model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve = 0\n",
    "        torch.save(final_model.state_dict(), f'../best_models/{data_hoje}/best_model.pth')\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    if no_improve >= patience:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break"
   ],
   "id": "ceff9fcc82f30ca",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-14 10:40:06,050] A new study created in memory with name: no-name-28b94a7f-4eaf-49c9-b369-5ef5d048589c\n",
      "[I 2024-08-14 10:41:16,761] Trial 0 finished with value: 0.2220389230116721 and parameters: {'hidden_size': 34, 'num_layers': 2, 'dropout': 0.40069303125942135, 'learning_rate': 0.002079717611368995, 'batch_size': 128, 'weight_decay': 2.405878791056044e-05}. Best is trial 0 with value: 0.2220389230116721.\n",
      "[I 2024-08-14 10:43:00,641] Trial 1 finished with value: 0.25118544914068713 and parameters: {'hidden_size': 240, 'num_layers': 2, 'dropout': 0.5580542313613788, 'learning_rate': 0.0017655807159163873, 'batch_size': 128, 'weight_decay': 0.00010072993364342317}. Best is trial 0 with value: 0.2220389230116721.\n",
      "[I 2024-08-14 10:47:21,794] Trial 2 finished with value: 0.16410353275076037 and parameters: {'hidden_size': 466, 'num_layers': 4, 'dropout': 0.2971638901398904, 'learning_rate': 0.009022550593338795, 'batch_size': 128, 'weight_decay': 3.482675019169891e-05}. Best is trial 2 with value: 0.16410353275076037.\n",
      "[I 2024-08-14 10:49:08,375] Trial 3 finished with value: 0.22941923526025587 and parameters: {'hidden_size': 204, 'num_layers': 2, 'dropout': 0.1659209468304565, 'learning_rate': 0.0035731848682852545, 'batch_size': 128, 'weight_decay': 0.0019492855663030102}. Best is trial 2 with value: 0.16410353275076037.\n",
      "[I 2024-08-14 10:50:19,132] Trial 4 finished with value: 0.2530048458326247 and parameters: {'hidden_size': 111, 'num_layers': 2, 'dropout': 0.36911954694559346, 'learning_rate': 0.0005896580732103749, 'batch_size': 128, 'weight_decay': 6.954055999900714e-05}. Best is trial 2 with value: 0.16410353275076037.\n",
      "C:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2842627785667508 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "[I 2024-08-14 10:50:21,340] Trial 5 pruned. \n",
      "[I 2024-08-14 10:50:23,677] Trial 6 pruned. \n",
      "[I 2024-08-14 10:50:24,804] Trial 7 pruned. \n",
      "[I 2024-08-14 10:51:53,185] Trial 8 finished with value: 0.25307271124855163 and parameters: {'hidden_size': 83, 'num_layers': 3, 'dropout': 0.013335976577219688, 'learning_rate': 0.0017497529807949498, 'batch_size': 128, 'weight_decay': 1.813898960725406e-05}. Best is trial 2 with value: 0.16410353275076037.\n",
      "C:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4173250802895 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "[I 2024-08-14 10:51:55,303] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "Value:  0.16410353275076037\n",
      "Params: \n",
      "    hidden_size: 466\n",
      "    num_layers: 4\n",
      "    dropout: 0.2971638901398904\n",
      "    learning_rate: 0.009022550593338795\n",
      "    batch_size: 128\n",
      "    weight_decay: 3.482675019169891e-05\n",
      "Epoch 1/500, Validation Loss: 0.2152\n",
      "Epoch 2/500, Validation Loss: 0.1928\n",
      "Epoch 3/500, Validation Loss: 0.1912\n",
      "Epoch 4/500, Validation Loss: 0.1685\n",
      "Epoch 5/500, Validation Loss: 0.1749\n",
      "Epoch 6/500, Validation Loss: 0.1760\n",
      "Epoch 7/500, Validation Loss: 0.1688\n",
      "Epoch 8/500, Validation Loss: 0.1705\n",
      "Epoch 9/500, Validation Loss: 0.1667\n",
      "Epoch 10/500, Validation Loss: 0.1804\n",
      "Epoch 11/500, Validation Loss: 0.1630\n",
      "Epoch 12/500, Validation Loss: 0.1734\n",
      "Epoch 13/500, Validation Loss: 0.1749\n",
      "Epoch 14/500, Validation Loss: 0.1674\n",
      "Epoch 15/500, Validation Loss: 0.1998\n",
      "Epoch 16/500, Validation Loss: 0.1709\n",
      "Epoch 17/500, Validation Loss: 0.1631\n",
      "Epoch 18/500, Validation Loss: 0.1638\n",
      "Epoch 19/500, Validation Loss: 0.1764\n",
      "Epoch 20/500, Validation Loss: 0.1638\n",
      "Epoch 21/500, Validation Loss: 0.1612\n",
      "Epoch 22/500, Validation Loss: 0.1625\n",
      "Epoch 23/500, Validation Loss: 0.1747\n",
      "Epoch 24/500, Validation Loss: 0.1668\n",
      "Epoch 25/500, Validation Loss: 0.1625\n",
      "Epoch 26/500, Validation Loss: 0.1723\n",
      "Epoch 27/500, Validation Loss: 0.1968\n",
      "Epoch 28/500, Validation Loss: 0.1656\n",
      "Epoch 29/500, Validation Loss: 0.1735\n",
      "Epoch 30/500, Validation Loss: 0.1829\n",
      "Epoch 31/500, Validation Loss: 0.1658\n",
      "Epoch 32/500, Validation Loss: 0.1767\n",
      "Epoch 33/500, Validation Loss: 0.1758\n",
      "Epoch 34/500, Validation Loss: 0.2081\n",
      "Epoch 35/500, Validation Loss: 0.1702\n",
      "Epoch 36/500, Validation Loss: 0.1897\n",
      "Epoch 37/500, Validation Loss: 0.1757\n",
      "Epoch 38/500, Validation Loss: 0.1794\n",
      "Epoch 39/500, Validation Loss: 0.1875\n",
      "Epoch 40/500, Validation Loss: 0.1654\n",
      "Epoch 41/500, Validation Loss: 0.1702\n",
      "Early stopping triggered after 41 epochs\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T13:54:06.044801Z",
     "start_time": "2024-08-14T13:54:05.727619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "final_model.load_state_dict(torch.load(f'../best_models/{data_hoje}/best_model.pth'))\n",
    "\n",
    "test_loader = DataLoader(TimeSeriesDataset(X_test, y_test), batch_size=best_params['batch_size'])\n",
    "final_model.eval()\n",
    "test_loss = 0\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = final_model(batch_X)\n",
    "        test_loss += criterion(outputs, batch_y).item()\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "        actuals.extend(batch_y.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "predictions = preprocessing_scaler.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "actuals = preprocessing_scaler.inverse_transform(np.array(actuals).reshape(-1, 1))\n",
    "\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "r2 = r2_score(actuals, predictions)\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "print(f'Mean Absolute Error: {mae:.4f}')\n",
    "print(f'R2 Score: {r2:.4f}')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(actuals, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.title('BiLSTM Model: Actual vs Predicted PM2.5 Values')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PM2.5')\n",
    "plt.savefig(f'../plots/{data_hoje}/bilstm_results.png')\n",
    "plt.close()\n",
    "\n",
    "logging.info(f'Best hyperparameters: {best_params}')\n",
    "logging.info(f'Test Loss: {test_loss:.4f}')\n",
    "logging.info(f'Mean Squared Error: {mse:.4f}')\n",
    "logging.info(f'Mean Absolute Error: {mae:.4f}')\n",
    "logging.info(f'R2 Score: {r2:.4f}')\n",
    "\n",
    "fim_execucao = pd.Timestamp.now()\n",
    "logging.info(f'{fim_execucao} - Processo de otimização e treinamento do modelo BiLSTM concluído')\n",
    "logging.info(f'Tempo total de execução: {fim_execucao - inicio_execucao}')\n",
    "logging.info('-' * 50)"
   ],
   "id": "ce2d28fca0da4981",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1598\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (3948,1) doesn't match the broadcast shape (3948,99)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 20\u001B[0m\n\u001B[0;32m     17\u001B[0m test_loss \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(test_loader)\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTest Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 20\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocessing_scaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minverse_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredictions\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m actuals \u001B[38;5;241m=\u001B[39m preprocessing_scaler\u001B[38;5;241m.\u001B[39minverse_transform(np\u001B[38;5;241m.\u001B[39marray(actuals)\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m     23\u001B[0m mse \u001B[38;5;241m=\u001B[39m mean_squared_error(actuals, predictions)\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1107\u001B[0m, in \u001B[0;36mStandardScaler.inverse_transform\u001B[1;34m(self, X, copy)\u001B[0m\n\u001B[0;32m   1105\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1106\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwith_std:\n\u001B[1;32m-> 1107\u001B[0m         \u001B[43mX\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_\u001B[49m\n\u001B[0;32m   1108\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwith_mean:\n\u001B[0;32m   1109\u001B[0m         X \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmean_\n",
      "\u001B[1;31mValueError\u001B[0m: non-broadcastable output operand with shape (3948,1) doesn't match the broadcast shape (3948,99)"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
