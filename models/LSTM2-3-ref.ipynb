{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Melhor modelo LSTM padrao ate o momento R2 de .71",
   "id": "dddff4c9c88ebe58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T06:47:30.021773Z",
     "start_time": "2024-10-21T06:47:30.018021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from copy import deepcopy as dc\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "import os"
   ],
   "id": "1ab0f1299a77df43",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T06:47:31.367487Z",
     "start_time": "2024-10-21T06:47:30.046145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuração inicial\n",
    "data_hoje = datetime.now().strftime('%d-%m')\n",
    "inicio_execucao = pd.Timestamp.now()\n",
    "\n",
    "# Criando diretórios para logs e plots\n",
    "os.makedirs(f'../logs/{data_hoje}', exist_ok=True)\n",
    "os.makedirs(f'../plots/{data_hoje}', exist_ok=True)\n",
    "\n",
    "# Configuração do logging\n",
    "logging.basicConfig(filename=f'../logs/{data_hoje}/lstm_optuna.log', level=logging.INFO, format='- %(message)s')\n",
    "logging.info('-' * 50)\n",
    "logging.info(f'{inicio_execucao} - Iniciando o processo de otimização e treinamento do modelo LSTM')\n",
    "\n",
    "# Carregando e preparando os dados\n",
    "df_original = pd.read_csv('../dados_tratados/combinado/Piratininga/Piratininga_tratado_combinado.csv',\n",
    "                          usecols=['PM2.5', 'Data e Hora', 'PM10', 'Monóxido de Carbono', 'Dióxido de Enxofre',\n",
    "                                   'Dióxido de Nitrogênio', 'Temperatura', 'Velocidade do Vento', 'Umidade Relativa',\n",
    "                                   'Direção do Vento'], low_memory=False)\n",
    "\n",
    "df_original['Data e Hora'] = pd.to_datetime(df_original['Data e Hora'])\n",
    "df_original.set_index('Data e Hora', inplace=True)\n",
    "df_original.sort_index(inplace=True)\n",
    "\n",
    "colunas_selecionadas = ['PM2.5', 'PM10', 'Monóxido de Carbono', 'Umidade Relativa', 'Direção do Vento']\n",
    "logging.info(f\"Colunas selecionadas: {colunas_selecionadas}\")\n",
    "df = df_original[colunas_selecionadas]\n",
    "df = df.loc['2019-01-01':'2022-01-01']\n",
    "\n",
    "df = df.apply(pd.to_numeric, errors='coerce')"
   ],
   "id": "418fe42641446a0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T06:53:01.660205Z",
     "start_time": "2024-10-21T06:47:31.388817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# fazendo o logging de qual algoritmo de imputação foi utilizado\n",
    "def log_imputation(method_name, impute_function, df):\n",
    "    df_imputed = impute_function(df)\n",
    "    logging.info(f\"Imputação realizada usando: {method_name}\")\n",
    "    return df_imputed\n",
    "\n",
    "def random_forest_imputer(df):\n",
    "    imputer = IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=0)\n",
    "    df_imputed = imputer.fit_transform(df)\n",
    "    df_imputed = pd.DataFrame(df_imputed, columns=df.columns, index=df.index)\n",
    "    return df_imputed\n",
    "\n",
    "df_imputed = log_imputation('Random Forest', random_forest_imputer, df)\n",
    "\n",
    "logging.info(f\"Dados ausentes antes da imputação: {df.isna().sum()}\")\n",
    "logging.info(f\"Dados ausentes após a imputação: {df_imputed.isna().sum()}\")\n",
    "logging.info(f\"Dados totais: {len(df_imputed)}\")"
   ],
   "id": "65334a755512eced",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.virtualenvs/pm25-plots/lib/python3.10/site-packages/sklearn/impute/_iterative.py:825: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T06:53:03.317122Z",
     "start_time": "2024-10-21T06:53:02.516406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Preparando os dados para LSTM\n",
    "def prepare_dataframe_for_lstm(df, n_steps):\n",
    "    df = dc(df)\n",
    "    for col in colunas_selecionadas:\n",
    "        for i in range(1, n_steps + 1):\n",
    "            df[f'{col}(t-{i})'] = df[col].shift(i)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "lookback = 24\n",
    "shifted_df = prepare_dataframe_for_lstm(df_imputed, lookback)\n",
    "\n",
    "# Dividindo em conjuntos de treino, validação e teste\n",
    "train_size = int(len(shifted_df) * 0.7)\n",
    "val_size = int(len(shifted_df) * 0.15)\n",
    "\n",
    "train_df = shifted_df.iloc[:train_size]\n",
    "val_df = shifted_df.iloc[train_size:train_size + val_size]\n",
    "test_df = shifted_df.iloc[train_size + val_size:]\n",
    "\n",
    "# Normalizando os dados de forma correta\n",
    "scaler = StandardScaler()\n",
    "train_scaled = pd.DataFrame(scaler.fit_transform(train_df), columns=shifted_df.columns, index=train_df.index)\n",
    "val_scaled = pd.DataFrame(scaler.transform(val_df), columns=shifted_df.columns, index=val_df.index)\n",
    "test_scaled = pd.DataFrame(scaler.transform(test_df), columns=shifted_df.columns, index=test_df.index)\n",
    "\n",
    "X_train, y_train = train_scaled.iloc[:, len(colunas_selecionadas):].values, train_scaled.iloc[:, 0].values\n",
    "X_val, y_val = val_scaled.iloc[:, len(colunas_selecionadas):].values, val_scaled.iloc[:, 0].values\n",
    "X_test, y_test = test_scaled.iloc[:, len(colunas_selecionadas):].values, test_scaled.iloc[:, 0].values\n",
    "\n",
    "# Reshape para LSTM\n",
    "X_train = X_train.reshape((-1, lookback, len(colunas_selecionadas)))\n",
    "X_val = X_val.reshape((-1, lookback, len(colunas_selecionadas)))\n",
    "X_test = X_test.reshape((-1, lookback, len(colunas_selecionadas)))\n",
    "y_train = y_train.reshape((-1, 1))\n",
    "y_val = y_val.reshape((-1, 1))\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "\n",
    "# Convertendo para tensores PyTorch\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_val = torch.tensor(X_val).float()\n",
    "y_val = torch.tensor(y_val).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Dataset e DataLoader\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "\n",
    "# Modelo LSTM\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ],
   "id": "f7d2cab8b34d6d8d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n",
      "/tmp/ipykernel_15550/4070780158.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'{col}(t-{i})'] = df[col].shift(i)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T06:53:03.410282Z",
     "start_time": "2024-10-21T06:53:03.387174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_weights = nn.Parameter(torch.randn(hidden_size))\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        attention_scores = torch.tanh(torch.matmul(lstm_output, self.attention_weights))\n",
    "        attention_scores = F.softmax(attention_scores, dim=1)\n",
    "        weighted_output = torch.mul(lstm_output, attention_scores.unsqueeze(-1))\n",
    "        return weighted_output.sum(dim=1)\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, activation, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.num_layers = len(hidden_sizes)\n",
    "        self.activation = activation\n",
    "\n",
    "        self.lstm_layers = nn.ModuleList([\n",
    "            nn.LSTM(input_size if i == 0 else hidden_sizes[i - 1],\n",
    "                    hidden_sizes[i],\n",
    "                    num_layers=1,\n",
    "                    batch_first=True)\n",
    "            for i in range(len(hidden_sizes))\n",
    "        ])\n",
    "\n",
    "        self.batch_norm_layers = nn.ModuleList([\n",
    "            nn.BatchNorm1d(hidden_sizes[i])\n",
    "            for i in range(len(hidden_sizes))\n",
    "        ])\n",
    "\n",
    "        self.attention = Attention(hidden_sizes[-1])\n",
    "        self.dropout = nn.Dropout(dropout) if self.num_layers > 1 else nn.Identity()\n",
    "        self.fc = nn.Linear(hidden_sizes[-1], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        for i, (lstm, bn) in enumerate(zip(self.lstm_layers, self.batch_norm_layers)):\n",
    "            h0 = torch.zeros(1, batch_size, self.hidden_sizes[i]).to(x.device)\n",
    "            c0 = torch.zeros(1, batch_size, self.hidden_sizes[i]).to(x.device)\n",
    "            x, _ = lstm(x, (h0, c0))\n",
    "            x = x.permute(0, 2, 1)\n",
    "            x = bn(x)\n",
    "            x = x.permute(0, 2, 1)\n",
    "\n",
    "            if i < len(self.lstm_layers) - 1:\n",
    "                x = self.apply_activation(x)\n",
    "                x = self.dropout(x)\n",
    "\n",
    "        x = self.attention(x)\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "    def apply_activation(self, x):\n",
    "        activations = {\n",
    "            'relu': F.relu,\n",
    "            'sigmoid': F.sigmoid,\n",
    "            'leaky_relu': F.leaky_relu,\n",
    "            'elu': F.elu,\n",
    "            'swish': lambda x: x * F.sigmoid(x),\n",
    "            'mish': F.mish,\n",
    "            'gelu': F.gelu,\n",
    "            'tanh': F.tanh,\n",
    "            'softplus': F.softplus\n",
    "        }\n",
    "        return activations.get(self.activation, lambda x: x)(x)\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, mse_weight=0.7, mae_weight=0.3):\n",
    "        super().__init__()\n",
    "        self.mse_weight = mse_weight\n",
    "        self.mae_weight = mae_weight\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.mae_loss = nn.L1Loss()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        mse = self.mse_loss(predictions, targets)\n",
    "        mae = self.mae_loss(predictions, targets)\n",
    "        return self.mse_weight * mse + self.mae_weight * mae"
   ],
   "id": "8d8cbc463f878c02",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T06:53:03.477671Z",
     "start_time": "2024-10-21T06:53:03.461791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, device):\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "    best_val_mse = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_actual = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "                output = model(x_batch)\n",
    "                val_loss += criterion(output, y_batch).item()\n",
    "                val_predictions.extend(output.cpu().numpy().flatten())\n",
    "                val_actual.extend(y_batch.cpu().numpy().flatten())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_mse = mean_squared_error(val_actual, val_predictions)\n",
    "\n",
    "        scheduler.step(val_mse)\n",
    "\n",
    "        if val_mse < best_val_mse:\n",
    "            best_val_mse = val_mse\n",
    "            epochs_without_improvement = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping activated at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model"
   ],
   "id": "4bbba2f5ba37fc6b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T06:53:03.530140Z",
     "start_time": "2024-10-21T06:53:03.520887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def objective(trial):\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 4)\n",
    "    hidden_sizes = [trial.suggest_int(f'hidden_size_{i}', 32, 128) for i in range(num_layers)]\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'leaky_relu', 'elu', 'swish', 'mish', 'gelu'])\n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.5)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True)\n",
    "    mse_weight = trial.suggest_float('mse_weight', 0.5, 0.9)\n",
    "\n",
    "    train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "    val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    model = LSTM(len(colunas_selecionadas), hidden_sizes, activation, dropout).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = CustomLoss(mse_weight=mse_weight, mae_weight=1-mse_weight)\n",
    "\n",
    "    num_epochs = 1000\n",
    "    early_stopping_patience = 20\n",
    "\n",
    "    model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, early_stopping_patience, device)\n",
    "\n",
    "    model.eval()\n",
    "    val_predictions = []\n",
    "    val_actual = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "            output = model(x_batch)\n",
    "            val_predictions.extend(output.cpu().numpy().flatten())\n",
    "            val_actual.extend(y_batch.cpu().numpy().flatten())\n",
    "\n",
    "    val_mse = mean_squared_error(val_actual, val_predictions)\n",
    "    return val_mse"
   ],
   "id": "5b1ce00606d3df1",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-21T06:53:03.581156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Função para data augmentation em séries temporais\n",
    "def time_series_augmentation(X, y, num_augmentations=1, noise_level=0.05):\n",
    "    augmented_X = []\n",
    "    augmented_y = []\n",
    "    \n",
    "    for _ in range(num_augmentations):\n",
    "        noise = np.random.normal(0, noise_level, X.shape)\n",
    "        augmented_X.append(X + noise)\n",
    "        augmented_y.append(y)\n",
    "    \n",
    "    return np.concatenate([X] + augmented_X), np.concatenate([y] + augmented_y)\n",
    "\n",
    "# Aplicar data augmentation\n",
    "X_train_aug, y_train_aug = time_series_augmentation(X_train, y_train)\n",
    "\n",
    "# Atualizar o dataset de treinamento\n",
    "train_dataset = TimeSeriesDataset(X_train_aug, y_train_aug)\n",
    "val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "# Otimização e treinamento final\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "best_params = study.best_params\n",
    "logging.info(f\"Melhores hiperparâmetros: {best_params}\")\n",
    "\n",
    "# Treinamento final com os melhores hiperparâmetros\n",
    "best_hidden_sizes = [best_params[f'hidden_size_{i}'] for i in range(best_params['num_layers'])]\n",
    "best_batch_size = best_params['batch_size']\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=False, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_batch_size, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "final_model = LSTM(len(colunas_selecionadas), best_hidden_sizes, best_params['activation'], best_params['dropout']).to(\n",
    "    device)\n",
    "optimizer = torch.optim.AdamW(final_model.parameters(), lr=best_params['learning_rate'])\n",
    "criterion = CustomLoss(mse_weight=best_params['mse_weight'], mae_weight=1-best_params['mse_weight'])\n",
    "\n",
    "\n",
    "num_epochs = 1000\n",
    "patience = 20\n",
    "\n",
    "final_model = train_model(final_model, train_loader, val_loader, criterion, optimizer, num_epochs, patience, device)\n",
    "\n",
    "# Salvar o modelo final\n",
    "torch.save(final_model.state_dict(), f'../models/best_model_optuna_{data_hoje}.pth')\n",
    "\n",
    "\n",
    "# Avaliação final\n",
    "def evaluate(model, dataloader):\n",
    "    predictions = []\n",
    "    actual = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "            output = model(x_batch)\n",
    "            predictions.extend(output.cpu().numpy().flatten())\n",
    "            actual.extend(y_batch.cpu().numpy().flatten())\n",
    "    return np.array(predictions), np.array(actual)\n",
    "\n",
    "\n",
    "train_predictions, train_actual = evaluate(final_model, train_loader)\n",
    "val_predictions, val_actual = evaluate(final_model, val_loader)\n",
    "test_predictions, test_actual = evaluate(final_model, test_loader)"
   ],
   "id": "f608d404a5cd2463",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-21 03:53:03,662] A new study created in memory with name: no-name-4e576831-4998-4bc6-9e52-00ea59405fac\n",
      "/home/daniel/.virtualenvs/pm25-plots/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "[I 2024-10-21 03:53:35,809] Trial 0 finished with value: 0.4622766077518463 and parameters: {'num_layers': 3, 'hidden_size_0': 97, 'hidden_size_1': 36, 'hidden_size_2': 87, 'batch_size': 128, 'learning_rate': 0.002302283513802658, 'activation': 'mish', 'dropout': 0.4848559267460351, 'weight_decay': 2.9243103867130233e-05, 'mse_weight': 0.7551248927367946}. Best is trial 0 with value: 0.4622766077518463.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping activated at epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.virtualenvs/pm25-plots/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "[I 2024-10-21 03:54:53,012] Trial 1 finished with value: 0.5493386387825012 and parameters: {'num_layers': 3, 'hidden_size_0': 55, 'hidden_size_1': 65, 'hidden_size_2': 101, 'batch_size': 64, 'learning_rate': 0.006275840074463405, 'activation': 'elu', 'dropout': 0.4994206559928419, 'weight_decay': 0.002682982058869856, 'mse_weight': 0.683129567381582}. Best is trial 0 with value: 0.4622766077518463.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping activated at epoch 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.virtualenvs/pm25-plots/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "[I 2024-10-21 03:55:58,537] Trial 2 finished with value: 0.5354831218719482 and parameters: {'num_layers': 4, 'hidden_size_0': 120, 'hidden_size_1': 74, 'hidden_size_2': 75, 'hidden_size_3': 53, 'batch_size': 64, 'learning_rate': 0.004976300255660886, 'activation': 'swish', 'dropout': 0.11350609823554625, 'weight_decay': 0.0019242381753838617, 'mse_weight': 0.8586557997677675}. Best is trial 0 with value: 0.4622766077518463.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping activated at epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.virtualenvs/pm25-plots/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping activated at epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-21 03:57:36,116] Trial 3 finished with value: 0.5687335729598999 and parameters: {'num_layers': 2, 'hidden_size_0': 67, 'hidden_size_1': 122, 'batch_size': 32, 'learning_rate': 1.3884992491925882e-05, 'activation': 'leaky_relu', 'dropout': 0.4642005825272923, 'weight_decay': 5.148829249329601e-05, 'mse_weight': 0.8677641838759191}. Best is trial 0 with value: 0.4622766077518463.\n",
      "/home/daniel/.virtualenvs/pm25-plots/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "[I 2024-10-21 03:58:06,249] Trial 4 finished with value: 0.43197906017303467 and parameters: {'num_layers': 1, 'hidden_size_0': 33, 'batch_size': 256, 'learning_rate': 0.00013878329663621553, 'activation': 'relu', 'dropout': 0.09503880810338833, 'weight_decay': 0.00213362347676356, 'mse_weight': 0.8768610362219248}. Best is trial 4 with value: 0.43197906017303467.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping activated at epoch 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.virtualenvs/pm25-plots/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "[I 2024-10-21 03:59:06,897] Trial 5 finished with value: 1.4657655954360962 and parameters: {'num_layers': 3, 'hidden_size_0': 104, 'hidden_size_1': 110, 'hidden_size_2': 103, 'batch_size': 64, 'learning_rate': 0.0024864599471997877, 'activation': 'gelu', 'dropout': 0.1422686523960059, 'weight_decay': 0.00544012499925315, 'mse_weight': 0.8659911656707137}. Best is trial 4 with value: 0.43197906017303467.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping activated at epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.virtualenvs/pm25-plots/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "[I 2024-10-21 03:59:52,366] Trial 6 finished with value: 0.4802960157394409 and parameters: {'num_layers': 3, 'hidden_size_0': 40, 'hidden_size_1': 71, 'hidden_size_2': 49, 'batch_size': 128, 'learning_rate': 4.1852973775038637e-05, 'activation': 'swish', 'dropout': 0.02659628172607975, 'weight_decay': 0.00010928600175532888, 'mse_weight': 0.7796092297005488}. Best is trial 4 with value: 0.43197906017303467.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping activated at epoch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.virtualenvs/pm25-plots/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "[I 2024-10-21 04:00:46,520] Trial 7 finished with value: 0.6055420637130737 and parameters: {'num_layers': 3, 'hidden_size_0': 113, 'hidden_size_1': 43, 'hidden_size_2': 40, 'batch_size': 64, 'learning_rate': 0.0005733422589361632, 'activation': 'swish', 'dropout': 0.06514458288728503, 'weight_decay': 2.8685195875595468e-05, 'mse_weight': 0.546186482257984}. Best is trial 4 with value: 0.43197906017303467.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping activated at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.virtualenvs/pm25-plots/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "[I 2024-10-21 04:01:30,053] Trial 8 finished with value: 0.476763516664505 and parameters: {'num_layers': 2, 'hidden_size_0': 64, 'hidden_size_1': 69, 'batch_size': 128, 'learning_rate': 0.0063902050101081595, 'activation': 'relu', 'dropout': 0.2732635002153402, 'weight_decay': 0.0004321469455870633, 'mse_weight': 0.6417998575293731}. Best is trial 4 with value: 0.43197906017303467.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping activated at epoch 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/.virtualenvs/pm25-plots/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Desnormalização\n",
    "def inverse_transform_data(normalized_data):\n",
    "    # Criar um array de zeros com o mesmo número de colunas que os dados originais\n",
    "    dummy_array = np.zeros((len(normalized_data), len(shifted_df.columns)))\n",
    "\n",
    "    # Colocar os dados normalizados na primeira coluna (assumindo que é PM2.5)\n",
    "    dummy_array[:, 0] = normalized_data\n",
    "\n",
    "    # Aplicar a transformação inversa\n",
    "    denormalized_data = scaler.inverse_transform(dummy_array)\n",
    "\n",
    "    # Retornar apenas a primeira coluna, que contém os dados desnormalizados de interesse\n",
    "    return denormalized_data[:, 0]\n",
    "\n",
    "\n",
    "# Desnormalização das previsões e valores reais\n",
    "train_predictions = inverse_transform_data(train_predictions)\n",
    "val_predictions = inverse_transform_data(val_predictions)\n",
    "test_predictions = inverse_transform_data(test_predictions)\n",
    "\n",
    "# Para os valores reais, precisamos garantir que estamos usando os dados originais não normalizados\n",
    "train_actual = df_imputed['PM2.5'].values[:len(train_predictions)]\n",
    "val_actual = df_imputed['PM2.5'].values[len(train_predictions):len(train_predictions) + len(val_predictions)]\n",
    "test_actual = df_imputed['PM2.5'].values[-len(test_predictions):]\n",
    "\n",
    "\n",
    "# Cálculo das métricas\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    return rmse, mse, mae, r2, mape\n",
    "\n",
    "\n",
    "# Calcular métricas\n",
    "train_rmse, train_mse, train_mae, train_r2, train_mape = calculate_metrics(train_actual, train_predictions)\n",
    "val_rmse, val_mse, val_mae, val_r2, val_mape = calculate_metrics(val_actual, val_predictions)\n",
    "test_rmse, test_mse, test_mae, test_r2, test_mape = calculate_metrics(test_actual, test_predictions)\n",
    "\n",
    "# Logging dos resultados\n",
    "logging.info(\n",
    "    f\"Métricas de Treino: RMSE={train_rmse:.4f}, MSE={train_mse:.4f}, MAE={train_mae:.4f}, R2={train_r2:.4f}, MAPE={train_mape:.4f}\")\n",
    "logging.info(\n",
    "    f\"Métricas de Validação: RMSE={val_rmse:.4f}, MSE={val_mse:.4f}, MAE={val_mae:.4f}, R2={val_r2:.4f}, MAPE={val_mape:.4f}\")\n",
    "logging.info(\n",
    "    f\"Métricas de Teste: RMSE={test_rmse:.4f}, MSE={test_mse:.4f}, MAE={test_mae:.4f}, R2={test_r2:.4f}, MAPE={test_mape:.4f}\")\n",
    "\n",
    "print(\n",
    "    f\"Métricas de Treino: RMSE={test_rmse:.4f}, MSE={test_mse:.4f}, MAE={test_mae:.4f}, R2={test_r2:.4f}, MAPE={test_mape:.4f}\")"
   ],
   "id": "8ed03d304545ce81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plotagem dos resultados\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_actual, label='Actual PM2.5')\n",
    "plt.plot(train_predictions, label='Predicted PM2.5')\n",
    "plt.title('Treinamento: PM2.5 Real vs Previsto')\n",
    "plt.xlabel('Hora')\n",
    "plt.ylabel('PM2.5')\n",
    "plt.legend()\n",
    "plt.savefig(f'../plots/{data_hoje}/lstm_optuna_train_{data_hoje}.png')\n"
   ],
   "id": "60faf0cdcf264037",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dates = shifted_df.index[:len(train_actual)]\n",
    "val_dates = shifted_df.index[len(train_actual):len(train_actual) + len(val_actual)]\n",
    "test_dates = shifted_df.index[-len(test_actual):]\n",
    "\n",
    "\n",
    "def plot_results(actual, predicted, dates, title, filename):\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    plt.plot(dates, actual, label='Real', color='blue')\n",
    "    plt.plot(dates, predicted, label='Previsto', color='red')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('PM2.5')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))  # Mostrar a cada 3 meses\n",
    "\n",
    "    plt.gcf().autofmt_xdate()  # Rotacionar e alinhar os rótulos de data\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../plots/{data_hoje}/{filename}_{data_hoje}.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "plot_results(train_actual, train_predictions, train_dates, 'Treinamento: PM2.5 Real vs Previsto', 'lstm_optuna_train')\n",
    "plot_results(val_actual, val_predictions, val_dates, 'Validação: PM2.5 Real vs Previsto', 'lstm_optuna_val')\n",
    "plot_results(test_actual, test_predictions, test_dates, 'Teste: PM2.5 Real vs Previsto', 'lstm_optuna_test')\n",
    "\n",
    "fim_execucao = pd.Timestamp.now()\n",
    "tempo_execucao = fim_execucao - inicio_execucao\n",
    "logging.info(f\"\\nExecução finalizada em {fim_execucao}\")\n",
    "logging.info(f\"Tempo total de execução: {tempo_execucao}\")"
   ],
   "id": "19edab1f2a08d516",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_results_by_month(actual, predicted, dates, title_prefix, filename_prefix):\n",
    "    df = pd.DataFrame({'date': dates, 'actual': actual, 'predicted': predicted})\n",
    "    df.set_index('date', inplace=True)\n",
    "\n",
    "    grouped = df.groupby(pd.Grouper(freq='M'))\n",
    "\n",
    "    for name, group in grouped:\n",
    "        if len(group) > 0:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(group.index, group['actual'], label='Real', color='blue')\n",
    "            plt.plot(group.index, group['predicted'], label='Previsto', color='red')\n",
    "\n",
    "            month_year = name.strftime('%B %Y')\n",
    "            plt.title(f'{title_prefix} - {month_year}')\n",
    "            plt.xlabel('Data')\n",
    "            plt.ylabel('PM2.5')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d-%m'))\n",
    "            plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=5))\n",
    "\n",
    "            plt.gcf().autofmt_xdate()  # Rotacionar e alinhar os rótulos de data\n",
    "            plt.tight_layout()\n",
    "\n",
    "            month_filename = f'{filename_prefix}_{name.strftime(\"%Y_%m\")}_{data_hoje}.png'\n",
    "            plt.savefig(f'../plots/{data_hoje}/{month_filename}')\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "plot_results_by_month(train_actual, train_predictions, train_dates, 'Treinamento: PM2.5 Real vs Previsto',\n",
    "                      'lstm_optuna_train')\n",
    "plot_results_by_month(val_actual, val_predictions, val_dates, 'Validação: PM2.5 Real vs Previsto', 'lstm_optuna_val')\n",
    "plot_results_by_month(test_actual, test_predictions, test_dates, 'Teste: PM2.5 Real vs Previsto', 'lstm_optuna_test')"
   ],
   "id": "5d311845f2a60318",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
