{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Modelo CNN-LSTM com Otimização de Hiperparâmetros\n",
    "funcionando com PyTorch e Optuna"
   ],
   "id": "70270859dba0d6fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T20:57:55.130929Z",
     "start_time": "2024-08-29T20:57:51.598056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from copy import deepcopy as dc\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "data_hoje = datetime.now().strftime('%d-%m')\n",
    "inicio_execucao = pd.Timestamp.now()\n",
    "\n",
    "os.makedirs(f'../logs/{data_hoje}', exist_ok=True)\n",
    "os.makedirs(f'../plots/{data_hoje}', exist_ok=True)\n",
    "os.makedirs(f'../best_models/{data_hoje}', exist_ok=True)\n",
    "\n",
    "logging.basicConfig(filename=f'../logs/{data_hoje}/bilstm_optuna.log', level=logging.INFO, format='- %(message)s')\n",
    "logging.info('-' * 50)\n",
    "logging.info(f'{inicio_execucao} - Iniciando o processo de otimização e treinamento do modelo BiLSTM')\n",
    "\n",
    "df_original = pd.read_csv('../dados_tratados/combinado/Piratininga/Piratininga_tratado_combinado.csv',\n",
    "                          usecols=['PM2.5', 'Data e Hora', 'PM10', 'Monóxido de Carbono'], low_memory=False)\n",
    "\n",
    "df_original['Data e Hora'] = pd.to_datetime(df_original['Data e Hora'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_original.index = df_original['Data e Hora']\n",
    "df_original.sort_index(inplace=True)\n",
    "\n",
    "colunas_selecionadas = ['PM2.5', 'PM10', 'Monóxido de Carbono']\n",
    "logging.info(f\"Colunas selecionadas: {colunas_selecionadas}\")\n",
    "\n",
    "df = df_original[colunas_selecionadas]\n",
    "df = df.loc['2019-01-01':'2022-01-01']\n",
    "\n",
    "logging.info(f\"Período de análise: 2019-01-01 a 2022-01-01\")\n",
    "df = df.apply(pd.to_numeric, errors='coerce')"
   ],
   "id": "6e68fea9a740698b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T20:58:47.195968Z",
     "start_time": "2024-08-29T20:57:55.147479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def log_imputation(method_name, impute_function, df):\n",
    "    df_imputed = impute_function(df)\n",
    "    logging.info(f\"Imputação realizada usando: {method_name}\")\n",
    "    return df_imputed\n",
    "\n",
    "def Mice_imputer(df):\n",
    "    mice_imputer = IterativeImputer(\n",
    "        estimator=ExtraTreesRegressor(n_estimators=100, random_state=42),\n",
    "        max_iter=10,\n",
    "        random_state=42,\n",
    "        n_nearest_features=None,\n",
    "        initial_strategy='mean'\n",
    "    )\n",
    "    \n",
    "    df_imputed = pd.DataFrame(\n",
    "        mice_imputer.fit_transform(df),\n",
    "        columns=df.columns,\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "def mean_imputer(df):\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(df),\n",
    "        columns=df.columns,\n",
    "        index=df.index\n",
    "    )\n",
    "    return df_imputed\n",
    "\n",
    "def median_imputer(df):\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(df),\n",
    "        columns=df.columns,\n",
    "        index=df.index\n",
    "    )\n",
    "    return df_imputed\n",
    "\n",
    "def forward_fill_imputer(df):\n",
    "    df_imputed = df.fillna(method='ffill')\n",
    "    return df_imputed\n",
    "\n",
    "def backward_fill_imputer(df):\n",
    "    df_imputed = df.fillna(method='bfill')\n",
    "    return df_imputed\n",
    "\n",
    "def linear_interpolation_imputer(df):\n",
    "    df_imputed = df.interpolate(method='linear')\n",
    "    return df_imputed\n",
    "\n",
    "def exponential_smoothing_imputer(df):\n",
    "    df_imputed = df.ewm(alpha=0.5).mean()\n",
    "    return df_imputed\n",
    "\n",
    "def knn_imputer(df):\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    df_imputed = pd.DataFrame(\n",
    "        imputer.fit_transform(df),\n",
    "        columns=df.columns,\n",
    "        index=df.index\n",
    "    )\n",
    "    return df_imputed\n",
    "\n",
    "# df_imputed = log_imputation('Mean', mean_imputer, df)\n",
    "# df_imputed = log_imputation('Median', median_imputer, df)\n",
    "# df_imputed = log_imputation('Forward Fill', forward_fill_imputer, df)\n",
    "# df_imputed = log_imputation('Backward Fill', backward_fill_imputer, df)\n",
    "# df_imputed = log_imputation('Linear Interpolation', linear_interpolation_imputer, df)\n",
    "# df_imputed = log_imputation('Exponential Smoothing', exponential_smoothing_imputer, df)\n",
    "# df_imputed = log_imputation('KNN', knn_imputer, df)\n",
    "df_imputed = log_imputation('MICE', Mice_imputer, df)\n"
   ],
   "id": "653a60c672216ebb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:825: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T21:11:54.066559Z",
     "start_time": "2024-08-29T20:58:47.386664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_cyclical_features(df):\n",
    "    df['hour'] = df.index.hour\n",
    "    df['day'] = df.index.day\n",
    "    df['month'] = df.index.month\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "    df.drop(['hour', 'day', 'month', 'day_of_week'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "df_imputed = add_cyclical_features(df_imputed)\n",
    "\n",
    "logging.info(f\"Adicionadas features cíclicas\")\n",
    "\n",
    "def prepare_dataframe_for_lstm(df, n_steps):\n",
    "    df = dc(df)\n",
    "    for col in df.columns:\n",
    "        for i in range(1, n_steps + 1):\n",
    "            df[f'{col}(t-{i})'] = df[col].shift(i)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "lookback = 8\n",
    "shifted_df = prepare_dataframe_for_lstm(df_imputed, lookback)\n",
    "\n",
    "preprocessing_scaler = StandardScaler()\n",
    "preprocessing_scaler.fit(shifted_df)\n",
    "shifted_df_as_np = preprocessing_scaler.transform(shifted_df)\n",
    "\n",
    "X = shifted_df_as_np[:, len(colunas_selecionadas):]\n",
    "y = shifted_df_as_np[:, 0]\n",
    "\n",
    "X = dc(np.flip(X, axis=1))\n",
    "\n",
    "train_split = int(len(X) * 0.7)\n",
    "val_split = int(len(X) * 0.85)\n",
    "\n",
    "X_train, X_val, X_test = X[:train_split], X[train_split:val_split], X[val_split:]\n",
    "y_train, y_val, y_test = y[:train_split], y[train_split:val_split], y[val_split:]\n",
    "\n",
    "X_train = X_train.reshape((-1, lookback, X_train.shape[1] // lookback))\n",
    "X_val = X_val.reshape((-1, lookback, X_val.shape[1] // lookback))\n",
    "X_test = X_test.reshape((-1, lookback, X_test.shape[1] // lookback))\n",
    "y_train = y_train.reshape((-1, 1))\n",
    "y_val = y_val.reshape((-1, 1))\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_val = torch.tensor(X_val).float()\n",
    "y_val = torch.tensor(y_val).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, kernel_size, out_channels):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=out_channels, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(out_channels, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Change shape to (batch_size, input_size, sequence_length)\n",
    "        x = self.cnn(x)\n",
    "        x = x.permute(0, 2, 1)  # Change shape back to (batch_size, sequence_length, out_channels)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_size = trial.suggest_int('hidden_size', 16, 128)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 5)\n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [128])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True)\n",
    "    kernel_size = trial.suggest_categorical('kernel_size', [3, 5])\n",
    "    out_channels = trial.suggest_int('out_channels', 16, 64)\n",
    "\n",
    "    model = CNN_LSTM(input_size=X_train.shape[2], hidden_size=hidden_size,\n",
    "                     num_layers=num_layers, dropout=dropout,\n",
    "                     kernel_size=kernel_size, out_channels=out_channels).to(device)\n",
    "\n",
    "    criterion = nn.HuberLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    train_loader = DataLoader(TimeSeriesDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TimeSeriesDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        intermediate_value = val_loss\n",
    "        trial.report(intermediate_value, epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=2)\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('Value: ', trial.value)\n",
    "print('Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "\n",
    "best_params = study.best_params\n",
    "final_model = CNN_LSTM(input_size=X_train.shape[2], hidden_size=best_params['hidden_size'],\n",
    "                          num_layers=best_params['num_layers'], dropout=best_params['dropout'],\n",
    "                          kernel_size=best_params['kernel_size'], out_channels=best_params['out_channels']).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.HuberLoss()\n",
    "optimizer = optim.AdamW(final_model.parameters(), lr=best_params['learning_rate'],\n",
    "                        weight_decay=best_params['weight_decay'])\n",
    "\n",
    "scaler = GradScaler()\n",
    "num_workers = 0\n",
    "\n",
    "train_loader = DataLoader(TimeSeriesDataset(X_train, y_train), batch_size=best_params['batch_size'], shuffle=True,\n",
    "                          num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(TimeSeriesDataset(X_val, y_val), batch_size=best_params['batch_size'], num_workers=num_workers,\n",
    "                        pin_memory=True)\n",
    "\n",
    "num_epochs = 500\n",
    "best_val_loss = float('inf')\n",
    "patience = 50\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    final_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Usar precisão mista para forward pass\n",
    "        with autocast():\n",
    "            outputs = final_model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "        # Escalar a perda e fazer o backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    final_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            with autocast():\n",
    "                outputs = final_model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve = 0\n",
    "        torch.save(final_model.state_dict(), f'../best_models/{data_hoje}/best_model.pth')\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    if no_improve >= patience:\n",
    "        print(f'Early stopping triggered after {epoch + 1} epochs')\n",
    "        break"
   ],
   "id": "daadd374a754de28",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-29 17:58:47,587] A new study created in memory with name: no-name-aa5748a8-18b3-481e-86de-72b193f69968\n",
      "C:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.45799675946653634 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "[I 2024-08-29 17:59:55,423] Trial 0 finished with value: 0.24736639857292175 and parameters: {'hidden_size': 29, 'num_layers': 1, 'dropout': 0.45799675946653634, 'learning_rate': 0.0070241399842261056, 'batch_size': 128, 'weight_decay': 0.0007313241675339971, 'kernel_size': 3, 'out_channels': 51}. Best is trial 0 with value: 0.24736639857292175.\n",
      "[I 2024-08-29 18:07:22,334] Trial 1 finished with value: 0.22242229912550218 and parameters: {'hidden_size': 95, 'num_layers': 5, 'dropout': 0.4604863119293005, 'learning_rate': 0.0030573174243804036, 'batch_size': 128, 'weight_decay': 9.506893388559688e-05, 'kernel_size': 5, 'out_channels': 56}. Best is trial 1 with value: 0.22242229912550218.\n",
      "C:\\Users\\portes\\AppData\\Local\\Temp\\ipykernel_10372\\2564181523.py:176: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "C:\\Users\\portes\\AppData\\Local\\Temp\\ipykernel_10372\\2564181523.py:196: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "Value:  0.22242229912550218\n",
      "Params: \n",
      "    hidden_size: 95\n",
      "    num_layers: 5\n",
      "    dropout: 0.4604863119293005\n",
      "    learning_rate: 0.0030573174243804036\n",
      "    batch_size: 128\n",
      "    weight_decay: 9.506893388559688e-05\n",
      "    kernel_size: 5\n",
      "    out_channels: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\portes\\AppData\\Local\\Temp\\ipykernel_10372\\2564181523.py:210: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Validation Loss: 0.1711\n",
      "Epoch 2/500, Validation Loss: 0.1672\n",
      "Epoch 3/500, Validation Loss: 0.1644\n",
      "Epoch 4/500, Validation Loss: 0.1721\n",
      "Epoch 5/500, Validation Loss: 0.1626\n",
      "Epoch 6/500, Validation Loss: 0.1642\n",
      "Epoch 7/500, Validation Loss: 0.1666\n",
      "Epoch 8/500, Validation Loss: 0.1671\n",
      "Epoch 9/500, Validation Loss: 0.1640\n",
      "Epoch 10/500, Validation Loss: 0.1680\n",
      "Epoch 11/500, Validation Loss: 0.1657\n",
      "Epoch 12/500, Validation Loss: 0.1685\n",
      "Epoch 13/500, Validation Loss: 0.1736\n",
      "Epoch 14/500, Validation Loss: 0.1794\n",
      "Epoch 15/500, Validation Loss: 0.1722\n",
      "Epoch 16/500, Validation Loss: 0.1706\n",
      "Epoch 17/500, Validation Loss: 0.1740\n",
      "Epoch 18/500, Validation Loss: 0.1745\n",
      "Epoch 19/500, Validation Loss: 0.1775\n",
      "Epoch 20/500, Validation Loss: 0.1834\n",
      "Epoch 21/500, Validation Loss: 0.1810\n",
      "Epoch 22/500, Validation Loss: 0.1787\n",
      "Epoch 23/500, Validation Loss: 0.1891\n",
      "Epoch 24/500, Validation Loss: 0.1951\n",
      "Epoch 25/500, Validation Loss: 0.1897\n",
      "Epoch 26/500, Validation Loss: 0.1915\n",
      "Epoch 27/500, Validation Loss: 0.2074\n",
      "Epoch 28/500, Validation Loss: 0.2027\n",
      "Epoch 29/500, Validation Loss: 0.2010\n",
      "Epoch 30/500, Validation Loss: 0.2011\n",
      "Epoch 31/500, Validation Loss: 0.1994\n",
      "Epoch 32/500, Validation Loss: 0.2088\n",
      "Epoch 33/500, Validation Loss: 0.2034\n",
      "Epoch 34/500, Validation Loss: 0.2095\n",
      "Epoch 35/500, Validation Loss: 0.2013\n",
      "Epoch 36/500, Validation Loss: 0.2093\n",
      "Epoch 37/500, Validation Loss: 0.2033\n",
      "Epoch 38/500, Validation Loss: 0.1986\n",
      "Epoch 39/500, Validation Loss: 0.2072\n",
      "Epoch 40/500, Validation Loss: 0.2130\n",
      "Epoch 41/500, Validation Loss: 0.2048\n",
      "Epoch 42/500, Validation Loss: 0.2169\n",
      "Epoch 43/500, Validation Loss: 0.2137\n",
      "Epoch 44/500, Validation Loss: 0.2130\n",
      "Epoch 45/500, Validation Loss: 0.2115\n",
      "Epoch 46/500, Validation Loss: 0.2098\n",
      "Epoch 47/500, Validation Loss: 0.2201\n",
      "Epoch 48/500, Validation Loss: 0.2140\n",
      "Epoch 49/500, Validation Loss: 0.2107\n",
      "Epoch 50/500, Validation Loss: 0.2122\n",
      "Epoch 51/500, Validation Loss: 0.2162\n",
      "Epoch 52/500, Validation Loss: 0.2205\n",
      "Epoch 53/500, Validation Loss: 0.2282\n",
      "Epoch 54/500, Validation Loss: 0.2154\n",
      "Epoch 55/500, Validation Loss: 0.2179\n",
      "Early stopping triggered after 55 epochs\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-29T21:11:55.608103Z",
     "start_time": "2024-08-29T21:11:54.073050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "final_model.load_state_dict(torch.load(f'../best_models/{data_hoje}/best_model.pth'))\n",
    "\n",
    "test_loader = DataLoader(TimeSeriesDataset(X_test, y_test), batch_size=best_params['batch_size'])\n",
    "final_model.eval()\n",
    "test_loss = 0\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = final_model(batch_X)\n",
    "        test_loss += criterion(outputs, batch_y).item()\n",
    "        predictions.extend(outputs.cpu().numpy().squeeze())\n",
    "        actuals.extend(batch_y.cpu().numpy().squeeze())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# Desnormalização\n",
    "def inverse_transform_data(data):\n",
    "    if data.ndim == 1:\n",
    "        data = data.reshape(-1, 1)\n",
    "    dummies = np.zeros((data.shape[0], shifted_df_as_np.shape[1]))\n",
    "    dummies[:, 0] = data.ravel()  # Use ravel() to ensure 1D array\n",
    "    dummies = preprocessing_scaler.inverse_transform(dummies)\n",
    "    return dummies[:, 0]\n",
    "\n",
    "# Desnormalização\n",
    "predictions = inverse_transform_data(np.array(predictions))\n",
    "actuals = inverse_transform_data(np.array(actuals))\n",
    "\n",
    "# Rest of the code remains the same\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "r2 = r2_score(actuals, predictions)\n",
    "mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "print(f'Root Mean Squared Error: {rmse:.4f}')\n",
    "print(f'Mean Absolute Error: {mae:.4f}')\n",
    "print(f'R-squared: {r2:.4f}')\n",
    "print(f'Mean Absolute Percentage Error: {mape:.4f}')\n",
    "\n",
    "logging.info(f'Mean Squared Error: {mse:.4f}')\n",
    "logging.info(f'Root Mean Squared Error: {rmse:.4f}')\n",
    "logging.info(f'Mean Absolute Error: {mae:.4f}')\n",
    "logging.info(f'R-squared: {r2:.4f}')\n",
    "logging.info(f'Mean Absolute Percentage Error: {mape:.4f}')\n",
    "\n",
    "\n",
    "# Plotar resultados\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(actuals, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.title('Actual vs Predicted PM2.5 Values')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PM2.5')\n",
    "plt.legend()\n",
    "plt.savefig(f'../plots/{data_hoje}/actual_vs_predicted.png')\n",
    "plt.close()\n",
    "\n",
    "# Plotar scatter plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(actuals, predictions, alpha=0.5)\n",
    "plt.plot([min(actuals), max(actuals)], [min(actuals), max(actuals)], 'r--', lw=2)\n",
    "plt.xlabel('Actual PM2.5')\n",
    "plt.ylabel('Predicted PM2.5')\n",
    "plt.title('Actual vs Predicted PM2.5 Scatter Plot')\n",
    "plt.savefig(f'../plots/{data_hoje}/scatter_plot.png')\n",
    "plt.close()\n",
    "\n",
    "# Salvar resultados\n",
    "results = pd.DataFrame({'Actual': actuals, 'Predicted': predictions})\n",
    "results.to_csv(f'../results/{data_hoje}/predictions.csv', index=False)\n",
    "\n",
    "# Salvar métricas\n",
    "with open(f'../results/{data_hoje}/metrics.txt', 'w') as f:\n",
    "    f.write(f'Mean Squared Error: {mse:.4f}\\n')\n",
    "    f.write(f'Root Mean Squared Error: {rmse:.4f}\\n')\n",
    "    f.write(f'Mean Absolute Error: {mae:.4f}\\n')\n",
    "    f.write(f'R-squared: {r2:.4f}\\n')\n",
    "\n",
    "print(\"Análise concluída. Resultados salvos nos diretórios correspondentes.\")\n"
   ],
   "id": "ce2d28fca0da4981",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\portes\\AppData\\Local\\Temp\\ipykernel_10372\\1532132186.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  final_model.load_state_dict(torch.load(f'../best_models/{data_hoje}/best_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1522\n",
      "Mean Squared Error: 27.7366\n",
      "Root Mean Squared Error: 5.2666\n",
      "Mean Absolute Error: 3.1793\n",
      "R-squared: 0.6532\n",
      "Mean Absolute Percentage Error: 45.2617\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '..\\results\\29-08'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 76\u001B[0m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;66;03m# Salvar resultados\u001B[39;00m\n\u001B[0;32m     75\u001B[0m results \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mActual\u001B[39m\u001B[38;5;124m'\u001B[39m: actuals, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPredicted\u001B[39m\u001B[38;5;124m'\u001B[39m: predictions})\n\u001B[1;32m---> 76\u001B[0m \u001B[43mresults\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../results/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mdata_hoje\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/predictions.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     78\u001B[0m \u001B[38;5;66;03m# Salvar métricas\u001B[39;00m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../results/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_hoje\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/metrics.txt\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    327\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    328\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    329\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[0;32m    330\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    331\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[0;32m    332\u001B[0m     )\n\u001B[1;32m--> 333\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001B[0m, in \u001B[0;36mNDFrame.to_csv\u001B[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[0m\n\u001B[0;32m   3956\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ABCDataFrame) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_frame()\n\u001B[0;32m   3958\u001B[0m formatter \u001B[38;5;241m=\u001B[39m DataFrameFormatter(\n\u001B[0;32m   3959\u001B[0m     frame\u001B[38;5;241m=\u001B[39mdf,\n\u001B[0;32m   3960\u001B[0m     header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3964\u001B[0m     decimal\u001B[38;5;241m=\u001B[39mdecimal,\n\u001B[0;32m   3965\u001B[0m )\n\u001B[1;32m-> 3967\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataFrameRenderer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3968\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath_or_buf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3969\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlineterminator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlineterminator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3970\u001B[0m \u001B[43m    \u001B[49m\u001B[43msep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3971\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3972\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3973\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3974\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquoting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquoting\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3975\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3976\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindex_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3977\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3978\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3979\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquotechar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquotechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3980\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdate_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdate_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3981\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdoublequote\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdoublequote\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3982\u001B[0m \u001B[43m    \u001B[49m\u001B[43mescapechar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mescapechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3983\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3984\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001B[0m, in \u001B[0;36mDataFrameRenderer.to_csv\u001B[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[0m\n\u001B[0;32m    993\u001B[0m     created_buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    995\u001B[0m csv_formatter \u001B[38;5;241m=\u001B[39m CSVFormatter(\n\u001B[0;32m    996\u001B[0m     path_or_buf\u001B[38;5;241m=\u001B[39mpath_or_buf,\n\u001B[0;32m    997\u001B[0m     lineterminator\u001B[38;5;241m=\u001B[39mlineterminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1012\u001B[0m     formatter\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt,\n\u001B[0;32m   1013\u001B[0m )\n\u001B[1;32m-> 1014\u001B[0m \u001B[43mcsv_formatter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1016\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m created_buffer:\n\u001B[0;32m   1017\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path_or_buf, StringIO)\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001B[0m, in \u001B[0;36mCSVFormatter.save\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;124;03mCreate the writer & save.\u001B[39;00m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# apply compression and byte/text conversion\u001B[39;00m\n\u001B[1;32m--> 251\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Note: self.encoding is irrelevant here\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwriter \u001B[38;5;241m=\u001B[39m csvlib\u001B[38;5;241m.\u001B[39mwriter(\n\u001B[0;32m    261\u001B[0m         handles\u001B[38;5;241m.\u001B[39mhandle,\n\u001B[0;32m    262\u001B[0m         lineterminator\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlineterminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    267\u001B[0m         quotechar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquotechar,\n\u001B[0;32m    268\u001B[0m     )\n\u001B[0;32m    270\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save()\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\pandas\\io\\common.py:749\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;66;03m# Only for write methods\u001B[39;00m\n\u001B[0;32m    748\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m is_path:\n\u001B[1;32m--> 749\u001B[0m     \u001B[43mcheck_parent_directory\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    751\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m compression:\n\u001B[0;32m    752\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m compression \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzstd\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    753\u001B[0m         \u001B[38;5;66;03m# compression libraries do not like an explicit text-mode\u001B[39;00m\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\pandas\\io\\common.py:616\u001B[0m, in \u001B[0;36mcheck_parent_directory\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m    614\u001B[0m parent \u001B[38;5;241m=\u001B[39m Path(path)\u001B[38;5;241m.\u001B[39mparent\n\u001B[0;32m    615\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m parent\u001B[38;5;241m.\u001B[39mis_dir():\n\u001B[1;32m--> 616\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124mrf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot save file into a non-existent directory: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mOSError\u001B[0m: Cannot save file into a non-existent directory: '..\\results\\29-08'"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
