{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Melhor modelo LSTM padrao ate o momento R2 de .71",
   "id": "dddff4c9c88ebe58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T02:40:48.120773Z",
     "start_time": "2024-11-14T02:40:48.117063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import os\n",
    "from copy import deepcopy as dc\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "id": "1ab0f1299a77df43",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T02:40:48.490900Z",
     "start_time": "2024-11-14T02:40:48.173826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuração inicial\n",
    "data_hoje = datetime.now().strftime('%d-%m')\n",
    "inicio_execucao = pd.Timestamp.now()\n",
    "\n",
    "# Criando diretórios para logs e plots\n",
    "os.makedirs(f'../logs/{data_hoje}', exist_ok=True)\n",
    "os.makedirs(f'../plots/{data_hoje}', exist_ok=True)\n",
    "\n",
    "# Configuração do logging\n",
    "logging.basicConfig(filename=f'../logs/{data_hoje}/lstm_optuna.log', level=logging.INFO, format='- %(message)s')\n",
    "logging.info('-' * 50)\n",
    "logging.info(f'{inicio_execucao} - Iniciando o processo de otimização e treinamento do modelo LSTM')\n",
    "\n",
    "# Carregando e preparando os dados\n",
    "df_original = pd.read_csv('../dados_tratados/combinado/Piratininga/Piratininga_tratado_combinado.csv',\n",
    "                          usecols=['PM2.5', 'Data e Hora', 'PM10', 'Monóxido de Carbono', 'Dióxido de Enxofre',\n",
    "                                   'Dióxido de Nitrogênio', 'Temperatura', 'Velocidade do Vento', 'Umidade Relativa',\n",
    "                                   'Direção do Vento'], low_memory=False)\n",
    "\n",
    "df_original['Data e Hora'] = pd.to_datetime(df_original['Data e Hora'])\n",
    "df_original.set_index('Data e Hora', inplace=True)\n",
    "df_original.sort_index(inplace=True)\n",
    "\n",
    "colunas_selecionadas = ['PM2.5', 'PM10', 'Monóxido de Carbono']\n",
    "logging.info(f\"Colunas selecionadas: {colunas_selecionadas}\")\n",
    "df = df_original[colunas_selecionadas]\n",
    "df = df.loc['2019-01-01':'2022-01-01']\n",
    "\n",
    "df = df.apply(pd.to_numeric, errors='coerce')"
   ],
   "id": "418fe42641446a0",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T02:40:48.507616Z",
     "start_time": "2024-11-14T02:40:48.498004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# fazendo o logging de qual algoritmo de imputação foi utilizado\n",
    "def log_imputation(method_name, impute_function, df):\n",
    "    df_imputed = impute_function(df)\n",
    "    logging.info(f\"Imputação realizada usando: {method_name}\")\n",
    "    return df_imputed\n",
    "\n",
    "def linear_interpolation_imputer(df):\n",
    "    df_imputed = df.interpolate(method='linear')\n",
    "    return df_imputed\n",
    "\n",
    "\n",
    "def random_forest_imputer(df):\n",
    "    imputer = IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=0)\n",
    "    df_imputed = imputer.fit_transform(df)\n",
    "    df_imputed = pd.DataFrame(df_imputed, columns=df.columns, index=df.index)\n",
    "    return df_imputed\n",
    "\n",
    "\n",
    "# df_imputed = log_imputation('Random Forest', random_forest_imputer, df)\n",
    "df_imputed = log_imputation('linear', linear_interpolation_imputer, df)\n",
    "\n",
    "logging.info(f\"Dados ausentes antes da imputação: {df.isna().sum()}\")\n",
    "logging.info(f\"Dados ausentes após a imputação: {df_imputed.isna().sum()}\")\n",
    "logging.info(f\"Dados totais: {len(df_imputed)}\")"
   ],
   "id": "65334a755512eced",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T02:40:48.639057Z",
     "start_time": "2024-11-14T02:40:48.551921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "# Preparando os dados para LSTM\n",
    "def prepare_dataframe_for_lstm(df, n_steps, weekly_step):\n",
    "    df = dc(df)\n",
    "    for col in colunas_selecionadas:\n",
    "        # Adicionar as últimas `n_steps` horas\n",
    "        for i in range(1, n_steps + 1):\n",
    "            df[f'{col}(t-{i})'] = df[col].shift(i)\n",
    "        # Adicionar o mesmo dia da semana anterior\n",
    "        df[f'{col}(t-{weekly_step})'] = df[col].shift(weekly_step)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "# Parâmetros de janela\n",
    "lookback = 24  # 48 horas\n",
    "weekly_step = 24 * 7  # 7 dias\n",
    "\n",
    "# Preparar o dataframe com a nova configuração de janelas\n",
    "shifted_df = prepare_dataframe_for_lstm(df_imputed, lookback, weekly_step)\n",
    "shifted_df = shifted_df[[col for col in shifted_df.columns if any(c in col for c in colunas_selecionadas)]]\n",
    "\n",
    "\n",
    "# Dividindo em conjuntos de treino, validação e teste\n",
    "train_size = int(len(shifted_df) * 0.7)\n",
    "val_size = int(len(shifted_df) * 0.15)\n",
    "\n",
    "train_df = shifted_df.iloc[:train_size]\n",
    "val_df = shifted_df.iloc[train_size:train_size + val_size]\n",
    "test_df = shifted_df.iloc[train_size + val_size:]\n",
    "\n",
    "# Normalizando os dados de forma correta\n",
    "scaler = StandardScaler()\n",
    "train_scaled = pd.DataFrame(scaler.fit_transform(train_df), columns=shifted_df.columns, index=train_df.index)\n",
    "val_scaled = pd.DataFrame(scaler.transform(val_df), columns=shifted_df.columns, index=val_df.index)\n",
    "test_scaled = pd.DataFrame(scaler.transform(test_df), columns=shifted_df.columns, index=test_df.index)\n",
    "\n",
    "X_train, y_train = train_scaled.iloc[:, len(colunas_selecionadas):].values, train_scaled.iloc[:, 0].values\n",
    "X_val, y_val = val_scaled.iloc[:, len(colunas_selecionadas):].values, val_scaled.iloc[:, 0].values\n",
    "X_test, y_test = test_scaled.iloc[:, len(colunas_selecionadas):].values, test_scaled.iloc[:, 0].values\n",
    "\n",
    "# Reshape para LSTM\n",
    "X_train = X_train.reshape((-1, 25, len(colunas_selecionadas)))\n",
    "X_val = X_val.reshape((-1, 25, len(colunas_selecionadas)))\n",
    "X_test = X_test.reshape((-1, 25, len(colunas_selecionadas)))\n",
    "y_train = y_train.reshape((-1, 1))\n",
    "y_val = y_val.reshape((-1, 1))\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "\n",
    "# Convertendo para tensores PyTorch\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Dataset e DataLoader\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "\n",
    "# Modelo LSTM\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ],
   "id": "f7d2cab8b34d6d8d",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T02:40:48.859705Z",
     "start_time": "2024-11-14T02:40:48.691288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FeatureAttention(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        # Inicialização dos pesos baseada nas correlações conhecidas\n",
    "        initial_weights = torch.tensor([1.0, 0.491396, 0.339309])\n",
    "        self.feature_weights = nn.Parameter(initial_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Aplica pesos de atenção específicos para cada feature\n",
    "        return x * self.feature_weights.view(1, 1, -1)\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        # Calcula pesos de atenção temporal\n",
    "        attention_weights = self.attention(lstm_output)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)\n",
    "        return attention_weights * lstm_output\n",
    "\n",
    "\n",
    "\n",
    "class ImprovedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_attention = FeatureAttention(input_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        self.temporal_attention = TemporalAttention(hidden_size * 2)\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Guarda as features originais para uso na loss function\n",
    "        original_features = x[:, -1, :]  # Último timestep\n",
    "\n",
    "        # Aplica atenção nas features\n",
    "        x = self.feature_attention(x)\n",
    "\n",
    "        # Processa através do LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        # Aplica atenção temporal\n",
    "        attended_output = self.temporal_attention(lstm_out)\n",
    "\n",
    "        # Extrai o último estado com atenção\n",
    "        last_output = attended_output[:, -1, :]\n",
    "\n",
    "        # Processa através das camadas fully connected com batch norm\n",
    "        fc1_out = self.fc_layers[0](last_output)\n",
    "        fc1_out = self.bn1(fc1_out)\n",
    "        fc1_out = F.relu(fc1_out)\n",
    "        fc1_out = self.fc_layers[2](fc1_out)\n",
    "\n",
    "        fc2_out = self.fc_layers[3](fc1_out)\n",
    "        fc2_out = self.bn2(fc2_out)\n",
    "        fc2_out = F.relu(fc2_out)\n",
    "        fc2_out = self.fc_layers[5](fc2_out)\n",
    "\n",
    "        # Camada final\n",
    "        output = self.fc_layers[6](fc2_out)\n",
    "\n",
    "        return output, original_features\n",
    "\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, correlation_weights=[1.0, 0.491396, 0.339309]):\n",
    "        super().__init__()\n",
    "        self.correlation_weights = torch.tensor(correlation_weights)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.mae = nn.L1Loss()\n",
    "\n",
    "    def calculate_correlation(self, x, y):\n",
    "        \"\"\"Calculate correlation between two vectors\"\"\"\n",
    "        x_centered = x - x.mean()\n",
    "        y_centered = y - y.mean()\n",
    "\n",
    "        numerator = (x_centered * y_centered).sum()\n",
    "        denominator = torch.sqrt((x_centered ** 2).sum() * (y_centered ** 2).sum())\n",
    "\n",
    "        return numerator / (denominator + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "\n",
    "    def forward(self, y_pred, y_true, features=None):\n",
    "        # Debugging the types of y_pred and y_true\n",
    "        # print(f\"y_pred type: {type(y_pred)}, y_true type: {type(y_true)}\")\n",
    "        assert isinstance(y_pred, torch.Tensor), \"y_pred must be a PyTorch tensor\"\n",
    "        assert isinstance(y_true, torch.Tensor), \"y_true must be a PyTorch tensor\"\n",
    "\n",
    "        if y_pred.dim() == 1:\n",
    "            y_pred = y_pred.unsqueeze(1)\n",
    "        if y_true.dim() == 1:\n",
    "            y_true = y_true.unsqueeze(1)\n",
    "\n",
    "        mse_loss = self.mse(y_pred, y_true)\n",
    "        mae_loss = self.mae(y_pred, y_true)\n",
    "\n",
    "        # Perda base\n",
    "        base_loss = 0.7 * mse_loss + 0.3 * mae_loss\n",
    "\n",
    "        if features is not None:\n",
    "            try:\n",
    "                # Reshape predictions and features to ensure correct dimensions\n",
    "                y_pred_flat = y_pred.view(-1)\n",
    "\n",
    "                # Calculate correlations for each feature\n",
    "                correlations = []\n",
    "                for i in range(features.size(-1)):\n",
    "                    feature_flat = features[:, i].view(-1)\n",
    "                    correlation = self.calculate_correlation(y_pred_flat, feature_flat)\n",
    "                    correlations.append(correlation)\n",
    "\n",
    "                # Stack correlations into tensor\n",
    "                correlations = torch.stack(correlations)\n",
    "\n",
    "                # Calculate correlation loss\n",
    "                correlation_weights = self.correlation_weights.to(y_pred.device)\n",
    "                correlation_loss = torch.mean(torch.abs(correlations - correlation_weights))\n",
    "\n",
    "                return base_loss + 0.1 * correlation_loss\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error in correlation calculation: {e}\")\n",
    "                return base_loss\n",
    "\n",
    "        return base_loss"
   ],
   "id": "8d8cbc463f878c02",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T02:40:48.907625Z",
     "start_time": "2024-11-14T02:40:48.902562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=1000, patience=30):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    criterion = CustomLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions, _ = model(batch_x)\n",
    "            loss = criterion(predictions, batch_y, batch_x[:, -1, :])  # Usando último timestep das features\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validação\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                predictions, _ = model(batch_x)\n",
    "                val_loss += criterion(predictions, batch_y, batch_x[:, -1, :]).item()\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    return model\n"
   ],
   "id": "4bbba2f5ba37fc6b",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T03:43:21.791286Z",
     "start_time": "2024-11-14T02:40:48.950679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    \"\"\"Calcula o Mean Absolute Percentage Error\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred):\n",
    "    \"\"\"Calcula múltiplas métricas de avaliação\"\"\"\n",
    "    metrics = {\n",
    "        'MSE': mean_squared_error(y_true, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'MAPE': calculate_mape(y_true, y_pred),\n",
    "        'R2': r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "    # Adiciona correlação de Pearson\n",
    "    metrics['Pearson_Correlation'] = np.corrcoef(y_true.flatten(), y_pred.flatten())[0, 1]\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def plot_predictions(y_true, y_pred, title, save_path=None):\n",
    "    \"\"\"Cria visualizações das previsões vs valores reais\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plotar valores reais e previstos\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(y_true, label='Real', alpha=0.7)\n",
    "    plt.plot(y_pred, label='Previsto', alpha=0.7)\n",
    "    plt.title(f'{title} - Valores Reais vs Previstos')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plotar scatter plot\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', alpha=0.8)\n",
    "    plt.xlabel('Valores Reais')\n",
    "    plt.ylabel('Valores Previstos')\n",
    "    plt.title('Scatter Plot: Real vs Previsto')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Função objetivo para otimização com Optuna\"\"\"\n",
    "    config = {\n",
    "        'hidden_size': trial.suggest_int('hidden_size', 32, 256),\n",
    "        'num_layers': trial.suggest_int('num_layers', 1, 4),\n",
    "        'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128, 256]),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    }\n",
    "\n",
    "    train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "    val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    model = ImprovedLSTM(\n",
    "        input_size=X_train.shape[2],\n",
    "        hidden_size=config['hidden_size'],\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "    criterion = CustomLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 30\n",
    "\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions, _ = model(batch_x)\n",
    "            loss = criterion(predictions, batch_y, batch_x[:, -1, :])\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_true = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                predictions, features = model(batch_x)\n",
    "                val_loss += criterion(predictions, batch_y, features).item()\n",
    "\n",
    "                val_predictions.extend(predictions.cpu().numpy())\n",
    "                val_true.extend(batch_y.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "\n",
    "    val_metrics = evaluate_predictions(\n",
    "        np.array(val_true),\n",
    "        np.array(val_predictions)\n",
    "    )\n",
    "\n",
    "    for metric_name, metric_value in val_metrics.items():\n",
    "        trial.set_user_attr(metric_name, metric_value)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def train_and_evaluate_final_model(best_params, save_dir='resultados'):\n",
    "    \"\"\"Treina e avalia o modelo final com os melhores parâmetros\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Criar datasets\n",
    "    train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "    val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "    test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "    # Criar dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "    # Criar modelo final\n",
    "    model = ImprovedLSTM(\n",
    "        input_size=X_train.shape[2],\n",
    "        hidden_size=best_params['hidden_size'],\n",
    "        num_layers=best_params['num_layers'],\n",
    "        dropout=best_params['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    # Treinar modelo final\n",
    "    model = train_model(model, train_loader, val_loader)\n",
    "\n",
    "    # Avaliar em todos os conjuntos\n",
    "    results = {}\n",
    "    for name, loader in [('Train', train_loader), ('Validation', val_loader), ('Test', test_loader)]:\n",
    "        predictions = []\n",
    "        true_values = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                pred, _ = model(batch_x)\n",
    "                predictions.extend(pred.cpu().numpy())\n",
    "                true_values.extend(batch_y.cpu().numpy())\n",
    "\n",
    "        # Converter para arrays\n",
    "        predictions = np.array(predictions)\n",
    "        true_values = np.array(true_values)\n",
    "\n",
    "        # Calcular métricas\n",
    "        metrics = evaluate_predictions(true_values, predictions)\n",
    "        results[name] = metrics\n",
    "\n",
    "        # Criar visualizações\n",
    "        plot_predictions(\n",
    "            true_values,\n",
    "            predictions,\n",
    "            f'Conjunto {name}',\n",
    "            save_path=os.path.join(save_dir, f'predictions_{name.lower()}.png')\n",
    "        )\n",
    "\n",
    "    # Salvar resultados\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(os.path.join(save_dir, 'metrics.csv'))\n",
    "\n",
    "    # Salvar modelo\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'best_params': best_params,\n",
    "        'results': results\n",
    "    }, os.path.join(save_dir, 'model.pth'))\n",
    "\n",
    "    return model, results\n",
    "\n",
    "# Executar otimização\n",
    "def run_optimization(n_trials=50):\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Treinar modelo final com melhores parâmetros\n",
    "    best_params = study.best_trial.params\n",
    "    print(\"\\nMelhores hiperparâmetros encontrados:\")\n",
    "    print(best_params)\n",
    "\n",
    "    # Treinar e avaliar modelo final\n",
    "    final_model, results = train_and_evaluate_final_model(best_params)\n",
    "\n",
    "    # Imprimir resultados finais\n",
    "    print(\"\\nResultados finais:\")\n",
    "    for dataset_name, metrics in results.items():\n",
    "        print(f\"\\n{dataset_name}:\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"{metric_name}: {value:.4f}\")\n",
    "\n",
    "    return study, final_model, results\n",
    "\n",
    "# Executar otimização e treinamento\n",
    "if __name__ == \"__main__\":\n",
    "    study, final_model, results = run_optimization(n_trials=1)"
   ],
   "id": "db220aee2cf9e954",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-13 23:40:48,962] A new study created in memory with name: no-name-629a0e7c-a4b8-49ca-9917-72b9c63b12db\n",
      "[I 2024-11-14 00:23:06,787] Trial 0 finished with value: 0.4495667377486825 and parameters: {'hidden_size': 148, 'num_layers': 4, 'dropout': 0.16133087132712484, 'batch_size': 256, 'learning_rate': 0.00011513821359178125}. Best is trial 0 with value: 0.4495667377486825.\n",
      "C:\\dev\\fast_api\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'hidden_size': 148, 'num_layers': 4, 'dropout': 0.16133087132712484, 'batch_size': 256, 'learning_rate': 0.00011513821359178125}\n",
      "Early stopping at epoch 47\n",
      "\n",
      "Resultados finais:\n",
      "\n",
      "Train:\n",
      "MSE: 0.4017\n",
      "RMSE: 0.6338\n",
      "MAE: 0.3617\n",
      "MAPE: 127.3319\n",
      "R2: 0.5983\n",
      "Pearson_Correlation: 0.7740\n",
      "\n",
      "Validation:\n",
      "MSE: 0.4052\n",
      "RMSE: 0.6365\n",
      "MAE: 0.3967\n",
      "MAPE: 143.8007\n",
      "R2: 0.4731\n",
      "Pearson_Correlation: 0.6891\n",
      "\n",
      "Test:\n",
      "MSE: 0.3371\n",
      "RMSE: 0.5806\n",
      "MAE: 0.3496\n",
      "MAPE: 113.1328\n",
      "R2: 0.7331\n",
      "Pearson_Correlation: 0.8572\n"
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
