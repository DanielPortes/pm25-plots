{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# R2 de .73",
   "id": "dddff4c9c88ebe58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T02:11:28.766926Z",
     "start_time": "2024-11-24T02:11:28.762758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "import os\n",
    "from copy import deepcopy as dc\n",
    "from datetime import datetime\n",
    "import holidays\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "id": "1ab0f1299a77df43",
   "outputs": [],
   "execution_count": 244
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T02:11:28.869906Z",
     "start_time": "2024-11-24T02:11:28.865652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "id": "7e115da618db920b",
   "outputs": [],
   "execution_count": 245
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T02:11:29.093028Z",
     "start_time": "2024-11-24T02:11:28.871913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_hoje = datetime.now().strftime('%d-%m')\n",
    "inicio_execucao = pd.Timestamp.now()\n",
    "\n",
    "os.makedirs(f'../logs/{data_hoje}', exist_ok=True)\n",
    "os.makedirs(f'../plots/{data_hoje}', exist_ok=True)\n",
    "\n",
    "logging.basicConfig(filename=f'../logs/{data_hoje}/lstm_optuna.log', level=logging.INFO, format='- %(message)s')\n",
    "logging.info('-' * 50)\n",
    "logging.info(f'{inicio_execucao} - Iniciando o processo de otimização e treinamento do modelo LSTM')\n",
    "\n",
    "# Carregando e preparando os dados\n",
    "df_original = pd.read_csv('../dados_tratados/combinado/Piratininga/Piratininga_tratado_combinado.csv',\n",
    "                          usecols=['PM2.5', 'Data e Hora', 'PM10', 'Monóxido de Carbono', 'Dióxido de Enxofre',\n",
    "                                   'Dióxido de Nitrogênio', 'Temperatura', 'Velocidade do Vento', 'Umidade Relativa',\n",
    "                                   'Direção do Vento'], low_memory=False)\n",
    "\n",
    "df_original['Data e Hora'] = pd.to_datetime(df_original['Data e Hora'])\n",
    "df_original.set_index('Data e Hora', inplace=True)\n",
    "df_original.sort_index(inplace=True)\n",
    "\n",
    "colunas_selecionadas = ['PM2.5', 'PM10', 'Monóxido de Carbono']\n",
    "logging.info(f\"Colunas selecionadas: {colunas_selecionadas}\")\n",
    "df = df_original[colunas_selecionadas]\n",
    "df = df.loc[(df.index >= '2017-01-01') & ((df.index < '2020-01-01') | (df.index > '2021-09-30'))]\n",
    "\n",
    "df = df.apply(pd.to_numeric, errors='coerce')"
   ],
   "id": "418fe42641446a0",
   "outputs": [],
   "execution_count": 246
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T02:11:29.211945Z",
     "start_time": "2024-11-24T02:11:29.208012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_df(df, n_steps, weekly_step):\n",
    "    important_features = [\n",
    "        'PM2.5', 'PM10', 'Monóxido de Carbono'\n",
    "    ]\n",
    "\n",
    "    # Create a list to hold all the new columns\n",
    "    new_columns = []\n",
    "\n",
    "    for col in important_features:\n",
    "        if col in df.columns:\n",
    "            # Shift for n_steps\n",
    "            for i in range(1, n_steps + 1):\n",
    "                new_columns.append(df[col].shift(i).rename(f'{col}(t-{i})'))\n",
    "\n",
    "            # Shift for weekly_step (by hours)\n",
    "            for hour in range(24):\n",
    "                new_columns.append(df[col].shift(weekly_step - hour).rename(f'{col}(t-{weekly_step - hour})'))\n",
    "\n",
    "    # Concatenate the new columns at once\n",
    "    df = pd.concat([df] + new_columns, axis=1)\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ],
   "id": "a1b223f1c0cb3eb0",
   "outputs": [],
   "execution_count": 247
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T02:11:29.306002Z",
     "start_time": "2024-11-24T02:11:29.299973Z"
    }
   },
   "cell_type": "code",
   "source": "df_imputed = df.interpolate(method='linear')",
   "id": "65334a755512eced",
   "outputs": [],
   "execution_count": 248
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T02:11:29.575490Z",
     "start_time": "2024-11-24T02:11:29.389985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lookback = 24\n",
    "weekly_step = 24 * 7\n",
    "\n",
    "shifted_df = prepare_df(df_imputed, lookback, weekly_step)\n",
    "shifted_df = shifted_df[[col for col in shifted_df.columns if any(c in col for c in colunas_selecionadas)]]\n",
    "\n",
    "train_size = int(len(shifted_df) * 0.7)\n",
    "val_size = int(len(shifted_df) * 0.15)\n",
    "\n",
    "train_df = shifted_df.iloc[:train_size]\n",
    "val_df = shifted_df.iloc[train_size:train_size + val_size]\n",
    "test_df = shifted_df.iloc[train_size + val_size:]\n",
    "\n",
    "\n",
    "def apply_scaler(df_train, df_val, df_test, scaler_type='Standard'):\n",
    "    if scaler_type == 'Standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler_type == 'Robust':\n",
    "        scaler = RobustScaler()\n",
    "    else:\n",
    "        raise ValueError(\"scaler_type deve ser 'Standard' ou 'Robust'\")\n",
    "\n",
    "    train_scaled = pd.DataFrame(scaler.fit_transform(df_train), columns=df_train.columns, index=df_train.index)\n",
    "    val_scaled = pd.DataFrame(scaler.transform(df_val), columns=df_val.columns, index=df_val.index)\n",
    "    test_scaled = pd.DataFrame(scaler.transform(df_test), columns=df_test.columns, index=df_test.index)\n",
    "\n",
    "    return train_scaled, val_scaled, test_scaled\n",
    "\n",
    "\n",
    "train_scaled, val_scaled, test_scaled = apply_scaler(train_df, val_df, test_df, scaler_type='Robust')\n",
    "\n",
    "X_train, y_train = train_scaled.iloc[:, len(colunas_selecionadas):].values, train_scaled.iloc[:, 0].values\n",
    "X_val, y_val = val_scaled.iloc[:, len(colunas_selecionadas):].values, val_scaled.iloc[:, 0].values\n",
    "X_test, y_test = test_scaled.iloc[:, len(colunas_selecionadas):].values, test_scaled.iloc[:, 0].values\n",
    "\n",
    "X_train = X_train.reshape((-1, 48, len(colunas_selecionadas)))\n",
    "X_val = X_val.reshape((-1, 48, len(colunas_selecionadas)))\n",
    "X_test = X_test.reshape((-1, 48, len(colunas_selecionadas)))\n",
    "\n",
    "y_train = y_train.reshape((-1, 1))\n",
    "y_val = y_val.reshape((-1, 1))\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "y_train = y_train.reshape((-1, 1))\n",
    "y_val = y_val.reshape((-1, 1))\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "y_val = y_val.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ],
   "id": "f7d2cab8b34d6d8d",
   "outputs": [],
   "execution_count": 249
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T02:11:29.711864Z",
     "start_time": "2024-11-24T02:11:29.698905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FeatureAttention(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        initial_weights = torch.tensor([1.0, 0.491396, 0.339309])\n",
    "        self.feature_weights = nn.Parameter(initial_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.feature_weights.view(1, 1, -1)\n",
    "\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        attention_weights = self.attention(lstm_output)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)\n",
    "        return attention_weights * lstm_output\n",
    "\n",
    "\n",
    "class ImprovedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_attention = FeatureAttention(input_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        self.temporal_attention = TemporalAttention(hidden_size * 2)\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size // 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_features = x[:, -1, :]  # Último timestep\n",
    "\n",
    "        x = self.feature_attention(x)\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        attended_output = self.temporal_attention(lstm_out)\n",
    "\n",
    "        last_output = attended_output[:, -1, :]\n",
    "\n",
    "        fc1_out = self.fc_layers[0](last_output)\n",
    "        fc1_out = self.bn1(fc1_out)\n",
    "        fc1_out = F.relu(fc1_out)\n",
    "        fc1_out = self.fc_layers[2](fc1_out)\n",
    "\n",
    "        fc2_out = self.fc_layers[3](fc1_out)\n",
    "        fc2_out = self.bn2(fc2_out)\n",
    "        fc2_out = F.relu(fc2_out)\n",
    "        fc2_out = self.fc_layers[5](fc2_out)\n",
    "\n",
    "        output = self.fc_layers[6](fc2_out)\n",
    "\n",
    "        return output, original_features\n",
    "\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, correlation_weights=[1.0, 0.491396, 0.339309]):\n",
    "        super().__init__()\n",
    "        self.correlation_weights = torch.tensor(correlation_weights)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.mae = nn.L1Loss()\n",
    "\n",
    "    def calculate_correlation(self, x, y):\n",
    "        x_centered = x - x.mean()\n",
    "        y_centered = y - y.mean()\n",
    "\n",
    "        numerator = (x_centered * y_centered).sum()\n",
    "        denominator = torch.sqrt((x_centered ** 2).sum() * (y_centered ** 2).sum())\n",
    "\n",
    "        return numerator / (denominator + 1e-8)\n",
    "\n",
    "    def forward(self, y_pred, y_true, features=None):\n",
    "        assert isinstance(y_pred, torch.Tensor), \"y_pred must be a PyTorch tensor\"\n",
    "        assert isinstance(y_true, torch.Tensor), \"y_true must be a PyTorch tensor\"\n",
    "\n",
    "        if y_pred.dim() == 1:\n",
    "            y_pred = y_pred.unsqueeze(1)\n",
    "        if y_true.dim() == 1:\n",
    "            y_true = y_true.unsqueeze(1)\n",
    "\n",
    "        mse_loss = self.mse(y_pred, y_true)\n",
    "        mae_loss = self.mae(y_pred, y_true)\n",
    "\n",
    "        base_loss = 0.7 * mse_loss + 0.3 * mae_loss\n",
    "\n",
    "        if features is not None:\n",
    "            try:\n",
    "                y_pred_flat = y_pred.view(-1)\n",
    "\n",
    "                correlations = []\n",
    "                for i in range(features.size(-1)):\n",
    "                    feature_flat = features[:, i].view(-1)\n",
    "                    correlation = self.calculate_correlation(y_pred_flat, feature_flat)\n",
    "                    correlations.append(correlation)\n",
    "\n",
    "                correlations = torch.stack(correlations)\n",
    "\n",
    "                correlation_weights = self.correlation_weights.to(y_pred.device)\n",
    "                correlation_loss = torch.mean(torch.abs(correlations - correlation_weights))\n",
    "\n",
    "                return base_loss + 0.1 * correlation_loss\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error in correlation calculation: {e}\")\n",
    "                return base_loss\n",
    "\n",
    "        return base_loss"
   ],
   "id": "8d8cbc463f878c02",
   "outputs": [],
   "execution_count": 250
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T02:11:29.817407Z",
     "start_time": "2024-11-24T02:11:29.812634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, OneCycleLR\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=1000, patience=30):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, steps_per_epoch=len(train_loader),\n",
    "                                                    epochs=num_epochs)\n",
    "    criterion = CustomLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions, _ = model(batch_x)\n",
    "            loss = criterion(predictions, batch_y, batch_x[:, -1, :])  # Usando último timestep das features\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validação\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                predictions, _ = model(batch_x)\n",
    "                val_loss += criterion(predictions, batch_y, batch_x[:, -1, :]).item()\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    return model\n"
   ],
   "id": "4bbba2f5ba37fc6b",
   "outputs": [],
   "execution_count": 251
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T02:42:39.030700Z",
     "start_time": "2024-11-24T02:11:29.900901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred):\n",
    "    metrics = {\n",
    "        'MSE': mean_squared_error(y_true, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'MAPE': calculate_mape(y_true, y_pred),\n",
    "        'R2': r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "    metrics['Pearson_Correlation'] = np.corrcoef(y_true.flatten(), y_pred.flatten())[0, 1]\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_predictions(y_true, y_pred, title, save_path=None):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(y_true, label='Real', alpha=0.7)\n",
    "    plt.plot(y_pred, label='Previsto', alpha=0.7)\n",
    "    plt.title(f'{title} - Valores Reais vs Previstos')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plotar scatter plot\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', alpha=0.8)\n",
    "    plt.xlabel('Valores Reais')\n",
    "    plt.ylabel('Valores Previstos')\n",
    "    plt.title('Scatter Plot: Real vs Previsto')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    config = {\n",
    "        'hidden_size': trial.suggest_int('hidden_size', 32, 256),\n",
    "        'num_layers': trial.suggest_int('num_layers', 1, 4),\n",
    "        'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128, 256]),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    }\n",
    "\n",
    "    train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "    val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    model = ImprovedLSTM(\n",
    "        input_size=X_train.shape[2],\n",
    "        hidden_size=config['hidden_size'],\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "    criterion = CustomLoss().to(device)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 30\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions, _ = model(batch_x)\n",
    "            loss = criterion(predictions, batch_y, batch_x[:, -1, :])\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_true = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                predictions, features = model(batch_x)\n",
    "                val_loss += criterion(predictions, batch_y, features).item()\n",
    "\n",
    "                val_predictions.extend(predictions.cpu().numpy())\n",
    "                val_true.extend(batch_y.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "\n",
    "    val_metrics = evaluate_predictions(\n",
    "        np.array(val_true),\n",
    "        np.array(val_predictions)\n",
    "    )\n",
    "\n",
    "    for metric_name, metric_value in val_metrics.items():\n",
    "        trial.set_user_attr(metric_name, metric_value)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def train_and_evaluate_final_model(best_params, save_dir='resultados'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "    val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "    test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "\n",
    "    model = ImprovedLSTM(\n",
    "        input_size=X_train.shape[2],\n",
    "        hidden_size=best_params['hidden_size'],\n",
    "        num_layers=best_params['num_layers'],\n",
    "        dropout=best_params['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    model = train_model(model, train_loader, val_loader, num_epochs=1000, patience=50).to(device)\n",
    "\n",
    "    results = {}\n",
    "    for name, loader in [('Train', train_loader), ('Validation', val_loader), ('Test', test_loader)]:\n",
    "        predictions = []\n",
    "        true_values = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                pred, _ = model(batch_x)\n",
    "                predictions.extend(pred.cpu().numpy())\n",
    "                true_values.extend(batch_y.cpu().numpy())\n",
    "\n",
    "        predictions = np.array(predictions)\n",
    "        true_values = np.array(true_values)\n",
    "\n",
    "        metrics = evaluate_predictions(true_values, predictions)\n",
    "        results[name] = metrics\n",
    "\n",
    "        plot_predictions(\n",
    "            true_values,\n",
    "            predictions,\n",
    "            f'Conjunto {name}',\n",
    "            save_path=os.path.join(save_dir, f'predictions_{name.lower()}.png')\n",
    "        )\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(os.path.join(save_dir, 'metrics.csv'))\n",
    "\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'best_params': best_params,\n",
    "        'results': results\n",
    "    }, os.path.join(save_dir, 'model.pth'))\n",
    "\n",
    "    return model, results\n",
    "\n",
    "\n",
    "def run_optimization(n_trials=1):\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    print(\"\\nMelhores hiperparâmetros encontrados:\")\n",
    "    print(best_params)\n",
    "\n",
    "    # Treinar e avaliar modelo final\n",
    "    final_model, results = train_and_evaluate_final_model(best_params)\n",
    "\n",
    "    # Imprimir resultados finais\n",
    "    print(\"\\nResultados finais:\")\n",
    "    for dataset_name, metrics in results.items():\n",
    "        print(f\"\\n{dataset_name}:\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"{metric_name}: {value:.4f}\")\n",
    "\n",
    "    return study, final_model, results\n",
    "\n",
    "\n",
    "# Executar otimização e treinamento\n",
    "if __name__ == \"__main__\":\n",
    "    study, final_model, results = run_optimization(n_trials=3)"
   ],
   "id": "db220aee2cf9e954",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 23:11:29,912] A new study created in memory with name: no-name-fc1ea481-b3f0-4db1-9186-2b96cecf9a38\n",
      "[I 2024-11-23 23:18:36,393] Trial 0 finished with value: 0.4608844484405956 and parameters: {'hidden_size': 220, 'num_layers': 1, 'dropout': 0.1652521975004944, 'batch_size': 64, 'learning_rate': 0.000494718868899103}. Best is trial 0 with value: 0.4608844484405956.\n",
      "[I 2024-11-23 23:22:57,844] Trial 1 finished with value: 2.1314920403740625 and parameters: {'hidden_size': 235, 'num_layers': 2, 'dropout': 0.4083235444740241, 'batch_size': 256, 'learning_rate': 0.0001767671484571312}. Best is trial 0 with value: 0.4608844484405956.\n",
      "[I 2024-11-23 23:31:38,938] Trial 2 finished with value: 0.44726459253793477 and parameters: {'hidden_size': 109, 'num_layers': 3, 'dropout': 0.4420092809301226, 'batch_size': 32, 'learning_rate': 0.0006664415708947682}. Best is trial 2 with value: 0.44726459253793477.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhores hiperparâmetros encontrados:\n",
      "{'hidden_size': 109, 'num_layers': 3, 'dropout': 0.4420092809301226, 'batch_size': 32, 'learning_rate': 0.0006664415708947682}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\fast_api\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 72\n",
      "\n",
      "Resultados finais:\n",
      "\n",
      "Train:\n",
      "MSE: 0.7049\n",
      "RMSE: 0.8396\n",
      "MAE: 0.6915\n",
      "MAPE: 312.5441\n",
      "R2: 0.2951\n",
      "Pearson_Correlation: 0.7770\n",
      "\n",
      "Validation:\n",
      "MSE: 0.7518\n",
      "RMSE: 0.8671\n",
      "MAE: 0.7359\n",
      "MAPE: 203.1250\n",
      "R2: 0.0898\n",
      "Pearson_Correlation: 0.7295\n",
      "\n",
      "Test:\n",
      "MSE: 1.0056\n",
      "RMSE: 1.0028\n",
      "MAE: 0.8031\n",
      "MAPE: 246.2384\n",
      "R2: 0.3130\n",
      "Pearson_Correlation: 0.8021\n"
     ]
    }
   ],
   "execution_count": 252
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
