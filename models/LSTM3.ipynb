{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from copy import deepcopy as dc\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "import torch.nn.functional as F\n",
    "import os"
   ],
   "id": "1ab0f1299a77df43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configuração inicial\n",
    "data_hoje = datetime.now().strftime('%d-%m')\n",
    "inicio_execucao = pd.Timestamp.now()\n",
    "\n",
    "# Criando diretórios para logs e plots\n",
    "os.makedirs(f'../logs/{data_hoje}', exist_ok=True)\n",
    "os.makedirs(f'../plots/{data_hoje}', exist_ok=True)\n",
    "\n",
    "# Configuração do logging\n",
    "logging.basicConfig(filename=f'../logs/{data_hoje}/lstm_optuna.log', level=logging.INFO, format='- %(message)s')\n",
    "logging.info('-' * 50)\n",
    "logging.info(f'{inicio_execucao} - Iniciando o processo de otimização e treinamento do modelo LSTM')\n",
    "\n",
    "# Carregando e preparando os dados\n",
    "df_original = pd.read_csv('../dados_tratados/combinado/Piratininga/Piratininga_tratado_combinado.csv',\n",
    "                          usecols=['PM2.5', 'Data e Hora', 'PM10', 'Monóxido de Carbono'], low_memory=False)\n",
    "\n",
    "df_original['Data e Hora'] = pd.to_datetime(df_original['Data e Hora'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_original.index = df_original['Data e Hora']\n",
    "df_original.sort_index(inplace=True)\n",
    "\n",
    "colunas_selecionadas = ['PM2.5', 'PM10', 'Monóxido de Carbono']\n",
    "df = df_original[colunas_selecionadas]\n",
    "df = df.loc['2019-01-01':'2022-01-01']\n",
    "\n",
    "df = df.apply(pd.to_numeric, errors='coerce')"
   ],
   "id": "418fe42641446a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def treat_outliers(df):\n",
    "    for column in df.columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "        df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Função para imputação de dados ausentes\n",
    "def impute_missing_data(df):\n",
    "    # Imputação para dados ausentes aleatórios\n",
    "    random_imputer = SimpleImputer(strategy='mean')\n",
    "    df_random_imputed = pd.DataFrame(random_imputer.fit_transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "    # Imputação para dados ausentes em sequência (usando interpolação)\n",
    "    df_interpolated = df_random_imputed.interpolate(method='time')\n",
    "\n",
    "    # Imputação KNN para lidar com padrões complexos\n",
    "    knn_imputer = KNNImputer(n_neighbors=5)\n",
    "    df_knn_imputed = pd.DataFrame(knn_imputer.fit_transform(df_interpolated), columns=df.columns, index=df.index)\n",
    "\n",
    "    return df_interpolated\n",
    "\n",
    "\n",
    "# df = treat_outliers(df)\n",
    "df_imputed = impute_missing_data(df)\n",
    "\n",
    "logging.info(f\"Dados ausentes antes da imputação: {df.isna().sum()}\")\n",
    "logging.info(f\"Dados ausentes após a imputação: {df_imputed.isna().sum()}\")"
   ],
   "id": "65334a755512eced",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Preparando os dados para LSTM\n",
    "def prepare_dataframe_for_lstm(df, n_steps):\n",
    "    df = dc(df)\n",
    "    for col in colunas_selecionadas:\n",
    "        for i in range(1, n_steps + 1):\n",
    "            df[f'{col}(t-{i})'] = df[col].shift(i)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "lookback = 8  # 24 horas de lookback\n",
    "shifted_df = prepare_dataframe_for_lstm(df_imputed, lookback)\n",
    "\n",
    "# Normalizando os dados\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "shifted_df_as_np = scaler.fit_transform(shifted_df)\n",
    "\n",
    "X = shifted_df_as_np[:, len(colunas_selecionadas):]\n",
    "y = shifted_df_as_np[:, 0]  # Mantemos PM2.5 como nossa variável alvo\n",
    "\n",
    "X = dc(np.flip(X, axis=1))\n",
    "\n",
    "# Dividindo em conjuntos de treino, validação e teste\n",
    "train_split = int(len(X) * 0.7)\n",
    "val_split = int(len(X) * 0.85)\n",
    "\n",
    "X_train, X_val, X_test = X[:train_split], X[train_split:val_split], X[val_split:]\n",
    "y_train, y_val, y_test = y[:train_split], y[train_split:val_split], y[val_split:]\n",
    "\n",
    "# Reshape para LSTM\n",
    "X_train = X_train.reshape((-1, lookback, len(colunas_selecionadas)))\n",
    "X_val = X_val.reshape((-1, lookback, len(colunas_selecionadas)))\n",
    "X_test = X_test.reshape((-1, lookback, len(colunas_selecionadas)))\n",
    "y_train = y_train.reshape((-1, 1))\n",
    "y_val = y_val.reshape((-1, 1))\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "\n",
    "# Convertendo para tensores PyTorch\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_val = torch.tensor(X_val).float()\n",
    "y_val = torch.tensor(y_val).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Dataset e DataLoader\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "\n",
    "# Modelo LSTM\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ],
   "id": "dd40f0a27c8b6d44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import optuna\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to optimize\n",
    "    hidden_size = trial.suggest_int('hidden_size', 32, 256)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "\n",
    "    # Create model\n",
    "    model = LSTM(input_size=len(colunas_selecionadas), hidden_size=hidden_size, \n",
    "                 num_layers=num_layers, dropout=dropout).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(TimeSeriesDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TimeSeriesDataset(X_val, y_val), batch_size=batch_size)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(50):  # You can adjust the number of epochs\n",
    "        model.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y).item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Report intermediate objective value\n",
    "        intermediate_value = val_loss\n",
    "        trial.report(intermediate_value, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)  # You can adjust the number of trials\n",
    "\n",
    "# Print the best parameters and the best value\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "print('Value: ', trial.value)\n",
    "print('Params: ')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "best_params = study.best_params\n",
    "final_model = LSTM(input_size=len(colunas_selecionadas), \n",
    "                   hidden_size=best_params['hidden_size'],\n",
    "                   num_layers=best_params['num_layers'], \n",
    "                   dropout=best_params['dropout']).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "train_loader = DataLoader(TimeSeriesDataset(X_train, y_train), batch_size=best_params['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(TimeSeriesDataset(X_val, y_val), batch_size=best_params['batch_size'])\n",
    "\n",
    "# Training loop for the final model\n",
    "num_epochs = 100  # You can adjust this\n",
    "for epoch in range(num_epochs):\n",
    "    final_model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    final_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = final_model(batch_X)\n",
    "            val_loss += criterion(outputs, batch_y).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loader = DataLoader(TimeSeriesDataset(X_test, y_test), batch_size=best_params['batch_size'])\n",
    "final_model.eval()\n",
    "test_loss = 0\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = final_model(batch_X)\n",
    "        test_loss += criterion(outputs, batch_y).item()\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "        actuals.extend(batch_y.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# Inverse transform predictions and actuals\n",
    "predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "actuals = scaler.inverse_transform(np.array(actuals).reshape(-1, 1))\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "r2 = r2_score(actuals, predictions)\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "print(f'Mean Absolute Error: {mae:.4f}')\n",
    "print(f'R2 Score: {r2:.4f}')\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(actuals, label='Actual')\n",
    "plt.plot(predictions, label='Predicted')\n",
    "plt.legend()\n",
    "plt.title('LSTM Model: Actual vs Predicted PM2.5 Values')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('PM2.5')\n",
    "plt.savefig(f'../plots/{data_hoje}/lstm_results.png')\n",
    "plt.close()\n",
    "\n",
    "# Log results\n",
    "logging.info(f'Best hyperparameters: {best_params}')\n",
    "logging.info(f'Test Loss: {test_loss:.4f}')\n",
    "logging.info(f'Mean Squared Error: {mse:.4f}')\n",
    "logging.info(f'Mean Absolute Error: {mae:.4f}')\n",
    "logging.info(f'R2 Score: {r2:.4f}')\n",
    "\n",
    "fim_execucao = pd.Timestamp.now()\n",
    "logging.info(f'{fim_execucao} - Processo de otimização e treinamento do modelo LSTM concluído')\n",
    "logging.info(f'Tempo total de execução: {fim_execucao - inicio_execucao}')\n",
    "logging.info('-' * 50)"
   ],
   "id": "8d8cbc463f878c02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Desnormalização\n",
    "def inverse_transform_data(data):\n",
    "    dummies = np.zeros((data.shape[0], shifted_df_as_np.shape[1]))\n",
    "    dummies[:, 0] = data\n",
    "    dummies = scaler.inverse_transform(dummies)\n",
    "    return dummies[:, 0]\n",
    "\n",
    "\n",
    "train_predictions = inverse_transform_data(train_predictions)\n",
    "val_predictions = inverse_transform_data(val_predictions)\n",
    "test_predictions = inverse_transform_data(test_predictions)\n",
    "train_actual = inverse_transform_data(train_actual)\n",
    "val_actual = inverse_transform_data(val_actual)\n",
    "test_actual = inverse_transform_data(test_actual)\n",
    "\n",
    "\n",
    "# Cálculo das métricas\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return rmse, mse, mae, r2\n",
    "\n",
    "\n",
    "train_rmse, train_mse, train_mae, train_r2 = calculate_metrics(train_actual, train_predictions)\n",
    "val_rmse, val_mse, val_mae, val_r2 = calculate_metrics(val_actual, val_predictions)\n",
    "test_rmse, test_mse, test_mae, test_r2 = calculate_metrics(test_actual, test_predictions)"
   ],
   "id": "8ed03d304545ce81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Log das métricas finais\n",
    "logging.info(\"\\nMétricas finais:\")\n",
    "logging.info(\"Treinamento - RMSE: {:.4f}, MSE: {:.4f}, MAE: {:.4f}, R²: {:.4f}\".format(train_rmse, train_mse, train_mae, train_r2))\n",
    "logging.info(\"Validação - RMSE: {:.4f}, MSE: {:.4f}, MAE: {:.4f}, R²: {:.4f}\".format(val_rmse, val_mse, val_mae, val_r2))\n",
    "logging.info(\"Teste - RMSE: {:.4f}, MSE: {:.4f}, MAE: {:.4f}, R²: {:.4f}\".format(test_rmse, test_mse, test_mae, test_r2))\n",
    "\n",
    "# Plotagem dos resultados\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_actual, label='Actual PM2.5')\n",
    "plt.plot(train_predictions, label='Predicted PM2.5')\n",
    "plt.title('Treinamento: PM2.5 Real vs Previsto')\n",
    "plt.xlabel('Hora')\n",
    "plt.ylabel('PM2.5')\n",
    "plt.legend()\n",
    "plt.savefig(f'../plots/{data_hoje}/lstm_optuna_train_{data_hoje}.png')\n",
    "plt.close()"
   ],
   "id": "60faf0cdcf264037",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "train_dates = shifted_df.index[:len(train_actual)]\n",
    "val_dates = shifted_df.index[len(train_actual):len(train_actual) + len(val_actual)]\n",
    "test_dates = shifted_df.index[-len(test_actual):]\n",
    "\n",
    "def plot_results(actual, predicted, dates, title, filename):\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    plt.plot(dates, actual, label='Real', color='blue')\n",
    "    plt.plot(dates, predicted, label='Previsto', color='red')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('PM2.5')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))  # Mostrar a cada 3 meses\n",
    "\n",
    "    plt.gcf().autofmt_xdate()  # Rotacionar e alinhar os rótulos de data\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../plots/{data_hoje}/{filename}_{data_hoje}.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "plot_results(train_actual, train_predictions, train_dates, 'Treinamento: PM2.5 Real vs Previsto', 'lstm_optuna_train')\n",
    "plot_results(val_actual, val_predictions, val_dates, 'Validação: PM2.5 Real vs Previsto', 'lstm_optuna_val')\n",
    "plot_results(test_actual, test_predictions, test_dates, 'Teste: PM2.5 Real vs Previsto', 'lstm_optuna_test')\n",
    "\n",
    "fim_execucao = pd.Timestamp.now()\n",
    "tempo_execucao = fim_execucao - inicio_execucao\n",
    "logging.info(f\"\\nExecução finalizada em {fim_execucao}\")\n",
    "logging.info(f\"Tempo total de execução: {tempo_execucao}\")"
   ],
   "id": "19edab1f2a08d516",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "\n",
    "def plot_results_by_month(actual, predicted, dates, title_prefix, filename_prefix):\n",
    "    df = pd.DataFrame({'date': dates, 'actual': actual, 'predicted': predicted})\n",
    "    df.set_index('date', inplace=True)\n",
    "\n",
    "    grouped = df.groupby(pd.Grouper(freq='M'))\n",
    "\n",
    "    for name, group in grouped:\n",
    "        if len(group) > 0:  \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(group.index, group['actual'], label='Real', color='blue')\n",
    "            plt.plot(group.index, group['predicted'], label='Previsto', color='red')\n",
    "            \n",
    "            month_year = name.strftime('%B %Y')\n",
    "            plt.title(f'{title_prefix} - {month_year}')\n",
    "            plt.xlabel('Data')\n",
    "            plt.ylabel('PM2.5')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d-%m'))\n",
    "            plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=5))  \n",
    "\n",
    "            plt.gcf().autofmt_xdate()  # Rotacionar e alinhar os rótulos de data\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            month_filename = f'{filename_prefix}_{name.strftime(\"%Y_%m\")}_{data_hoje}.png'\n",
    "            plt.savefig(f'../plots/{data_hoje}/{month_filename}')\n",
    "            plt.close()\n",
    "\n",
    "plot_results_by_month(train_actual, train_predictions, train_dates, 'Treinamento: PM2.5 Real vs Previsto', 'lstm_optuna_train')\n",
    "plot_results_by_month(val_actual, val_predictions, val_dates, 'Validação: PM2.5 Real vs Previsto', 'lstm_optuna_val')\n",
    "plot_results_by_month(test_actual, test_predictions, test_dates, 'Teste: PM2.5 Real vs Previsto', 'lstm_optuna_test')"
   ],
   "id": "5d311845f2a60318",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
