{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-07T17:47:23.198400Z",
     "start_time": "2024-07-07T17:47:23.195375Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T17:47:23.376261Z",
     "start_time": "2024-07-07T17:47:23.199407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using {device}')\n",
    "\n",
    "df = pd.read_csv('../dados_tratados/combinado/Piratininga/Piratininga_tratado_combinado.csv',\n",
    "                 usecols=['PM2.5', 'Data e Hora'])\n",
    "df.dropna(inplace=True)\n",
    "df.index = pd.to_datetime(df['Data e Hora'], format='%Y-%m-%d %H:%M:%S')\n",
    "train_dates = pd.to_datetime(df['Data e Hora'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df['Data e Hora'] = pd.to_datetime(df['Data e Hora'])\n",
    "df['hour'] = df['Data e Hora'].dt.hour\n",
    "df['minute'] = df['Data e Hora'].dt.minute\n",
    "df['year'] = df['Data e Hora'].dt.year\n",
    "df['month'] = df['Data e Hora'].dt.month\n",
    "df['day'] = df['Data e Hora'].dt.day\n",
    "df['day_of_week'] = df['Data e Hora'].dt.dayofweek\n",
    "df['day_of_year'] = df['Data e Hora'].dt.dayofyear\n",
    "df['week'] = df['Data e Hora'].dt.isocalendar().week\n",
    "\n",
    "df.drop('Data e Hora', axis=1, inplace=True)"
   ],
   "id": "dcf1d1c8f7827e9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T17:47:23.558706Z",
     "start_time": "2024-07-07T17:47:23.376261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalizando os dados de PM2.5\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_values = scaler.fit_transform(df['PM2.5'].values.reshape(-1, 1))\n",
    "\n",
    "df['PM2.5'] = scaled_values\n",
    "\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data) - seq_length - 1):\n",
    "        x = data[i:(i + seq_length), :].astype(np.float32)\n",
    "        y = data[i + seq_length, 0]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "\n",
    "seq_length = 8\n",
    "X, y = create_sequences(df.values, seq_length)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float().reshape(-1, 1)\n",
    "X_val = torch.from_numpy(X_val).float()\n",
    "y_val = torch.from_numpy(y_val).float().reshape(-1, 1)\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float().reshape(-1, 1)\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ],
   "id": "5d2ffa1cf679a5a2",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-07T17:47:23.559713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# Definir input_size (precisa ser ajustado conforme seus dados)\n",
    "input_size = X_train.shape[2]  # Supondo que X_train seja um tensor com a forma (n_samples, seq_len, n_features)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_layer_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_layer_size, 1)\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        attention_weights = self.attention(lstm_output).squeeze(-1)\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "        context_vector = torch.sum(attention_weights.unsqueeze(-1) * lstm_output, dim=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class LSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, output_size, num_layers, drop_prob, activation_function):\n",
    "        super(LSTMWithAttention, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.attention = Attention(hidden_layer_size)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, _ = self.lstm(input_seq)\n",
    "        context_vector, attention_weights = self.attention(lstm_out)\n",
    "        predictions = self.linear(context_vector)\n",
    "        if self.activation_function == 'relu':\n",
    "            predictions = nn.ReLU()(predictions)\n",
    "        elif self.activation_function == 'tanh':\n",
    "            predictions = nn.Tanh()(predictions)\n",
    "        elif self.activation_function == 'sigmoid':\n",
    "            predictions = nn.Sigmoid()(predictions)\n",
    "        return predictions\n",
    "\n",
    "def train_and_evaluate_lstm(hidden_layer_size, num_layers, lr, batch_size, drop_prob, activation_function, weight_decay, num_epochs, patience):\n",
    "    model = LSTMWithAttention(\n",
    "        input_size=input_size,\n",
    "        hidden_layer_size=hidden_layer_size,\n",
    "        num_layers=num_layers,\n",
    "        drop_prob=drop_prob,\n",
    "        output_size=1,\n",
    "        activation_function=activation_function\n",
    "    ).to(device)\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    val_data = TensorDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    def train_model():\n",
    "        model.train()\n",
    "        for seq, labels in train_loader:\n",
    "            seq, labels = seq.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(seq)\n",
    "            single_loss = loss_function(y_pred, labels)\n",
    "            single_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def evaluate_model():\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for seq, labels in val_loader:\n",
    "                seq, labels = seq.to(device), labels.to(device)\n",
    "                y_pred = model(seq)\n",
    "                single_loss = loss_function(y_pred, labels)\n",
    "                val_loss += single_loss.item() * seq.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        return val_loss\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_model()\n",
    "        val_loss = evaluate_model()\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return val_loss\n",
    "\n",
    "class PyTorchLSTMAttentionRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, hidden_layer_size=100, num_layers=2, lr=0.001, batch_size=64, drop_prob=0.2,\n",
    "                 activation_function='relu', weight_decay=1e-8, num_epochs=50, patience=5):\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lr = lr\n",
    "        self.drop_prob = drop_prob\n",
    "        self.batch_size = batch_size\n",
    "        self.activation_function = activation_function\n",
    "        self.weight_decay = weight_decay\n",
    "        self.num_epochs = num_epochs\n",
    "        self.patience = patience\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = LSTMWithAttention(\n",
    "            input_size=input_size,\n",
    "            hidden_layer_size=self.hidden_layer_size,\n",
    "            num_layers=self.num_layers,\n",
    "            drop_prob=self.drop_prob,\n",
    "            output_size=1,\n",
    "            activation_function=self.activation_function\n",
    "        ).to(device)\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        train_data = TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(y).float().reshape(-1, 1))\n",
    "        self.train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.train_model()\n",
    "        return self\n",
    "\n",
    "    def train_model(self):\n",
    "        self.model.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for seq, labels in self.train_loader:\n",
    "                seq, labels = seq.to(device), labels.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                y_pred = self.model(seq)\n",
    "                single_loss = self.loss_function(y_pred, labels)\n",
    "                single_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        test_data = torch.from_numpy(X).float()\n",
    "        test_loader = DataLoader(test_data, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for seq in test_loader:\n",
    "                seq = seq.to(device)\n",
    "                y_pred = self.model(seq)\n",
    "                predictions.append(y_pred.cpu().numpy())\n",
    "\n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        return predictions\n",
    "\n",
    "param_space = {\n",
    "    'hidden_layer_size': Integer(64, 512),\n",
    "    'num_layers': Integer(1, 5),\n",
    "    'lr': Real(0.0001, 0.01, prior='log-uniform'),\n",
    "    'batch_size': Categorical([32, 64, 96, 128]),\n",
    "    'num_epochs': Categorical([100, 200, 300, 400]),\n",
    "    'activation_function': Categorical(['relu', 'tanh', 'sigmoid']),\n",
    "    'drop_prob': Categorical([0.1, 0.2, 0.3]),\n",
    "    'weight_decay': Categorical([1e-6, 1e-4, 0.01]),\n",
    "    'patience': Categorical([3, 10, 15])\n",
    "}\n",
    "\n",
    "bayes_search = BayesSearchCV(estimator=PyTorchLSTMAttentionRegressor(), search_spaces=param_space, scoring='neg_mean_squared_error', cv=3, n_iter=30, random_state=42, verbose=3)\n",
    "bayes_search.fit(X_train.numpy(), y_train.numpy())\n",
    "\n",
    "print(\"Best Parameters:\", bayes_search.best_params_)\n",
    "print(\"Best Score:\", -bayes_search.best_score_)\n"
   ],
   "id": "f6c8ce1bf2fea71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "68eeabc2719fb12d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
