{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Modelo LSTM\n",
    "3 colunas\n",
    "pytorch light\n",
    "optuna"
   ],
   "id": "3d3a70a7a3d2c88a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T13:23:25.968911Z",
     "start_time": "2024-11-24T13:23:21.124264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "id": "d14921d7ecd62577",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\fast_api\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "\nCould not find `optuna-integration` for `pytorch_lightning`.\nPlease run `pip install optuna-integration[pytorch_lightning]`.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[1;32mC:\\dev\\fast_api\\venv\\Lib\\site-packages\\optuna\\integration\\pytorch_lightning.py:5\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m----> 5\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01moptuna_integration\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpytorch_lightning\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PyTorchLightningPruningCallback\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m:\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'optuna_integration'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[1;32mC:\\dev\\fast_api\\venv\\Lib\\site-packages\\optuna\\integration\\__init__.py:132\u001B[0m, in \u001B[0;36m_IntegrationModule._get_module\u001B[1;34m(self, module_name)\u001B[0m\n\u001B[0;32m    131\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 132\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m.\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodule_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;18;43m__name__\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m:\n",
      "File \u001B[1;32m~\\scoop\\apps\\python\\current\\Lib\\importlib\\__init__.py:90\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m     89\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m---> 90\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1387\u001B[0m, in \u001B[0;36m_gcd_import\u001B[1;34m(name, package, level)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1360\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1331\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:935\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:995\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:488\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[1;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[1;32mC:\\dev\\fast_api\\venv\\Lib\\site-packages\\optuna\\integration\\pytorch_lightning.py:7\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m:\n\u001B[1;32m----> 7\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m(_INTEGRATION_IMPORT_ERROR_TEMPLATE\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpytorch_lightning\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m     10\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPyTorchLightningPruningCallback\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: \nCould not find `optuna-integration` for `pytorch_lightning`.\nPlease run `pip install optuna-integration[pytorch_lightning]`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpytorch_lightning\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mloggers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TensorBoardLogger\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01moptuna\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01moptuna\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mintegration\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PyTorchLightningPruningCallback\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatetime\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m datetime\n",
      "File \u001B[1;32mC:\\dev\\fast_api\\venv\\Lib\\site-packages\\optuna\\integration\\__init__.py:120\u001B[0m, in \u001B[0;36m_IntegrationModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m    118\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_module(name)\n\u001B[0;32m    119\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_class_to_module\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m--> 120\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_class_to_module\u001B[49m\u001B[43m[\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    121\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\dev\\fast_api\\venv\\Lib\\site-packages\\optuna\\integration\\__init__.py:134\u001B[0m, in \u001B[0;36m_IntegrationModule._get_module\u001B[1;34m(self, module_name)\u001B[0m\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m module_name, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m    133\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m:\n\u001B[1;32m--> 134\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m(_INTEGRATION_IMPORT_ERROR_TEMPLATE\u001B[38;5;241m.\u001B[39mformat(module_name))\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: \nCould not find `optuna-integration` for `pytorch_lightning`.\nPlease run `pip install optuna-integration[pytorch_lightning]`."
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# cria pasta de logs com data de hoje\n",
    "\n",
    "data_hoje = datetime.now().strftime('%d-%m')\n",
    "os.makedirs(f'../logs/{data_hoje}', exist_ok=True)\n",
    "os.makedirs(f'../plots/{data_hoje}', exist_ok=True)"
   ],
   "id": "d8b9a719f4528b69",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "inicio_execucao = pd.Timestamp.now()\n",
    "\n",
    "logging.basicConfig(filename=f'../logs/{data_hoje}/lstm.log', level=logging.INFO, format='- %(message)s')\n",
    "logging.info('-' * 50)\n",
    "logging.info(f'{inicio_execucao} - Iniciando o processo de treinamento do modelo LSTM')"
   ],
   "id": "7f1eda578a581056",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_original = pd.read_csv('../dados_tratados/combinado/Piratininga/Piratininga_tratado_combinado.csv',\n",
    "                          usecols=['PM2.5', 'Data e Hora', 'PM10', 'Monóxido de Carbono'], low_memory=False)"
   ],
   "id": "171955d3a27b508c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_original['Data e Hora'] = pd.to_datetime(df_original['Data e Hora'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_original.index = df_original['Data e Hora']\n",
    "df_original.sort_index(inplace=True)\n",
    "\n",
    "colunas_selecionadas = ['PM2.5', 'PM10', 'Monóxido de Carbono']\n",
    "df = df_original[colunas_selecionadas]\n",
    "\n",
    "df = df.loc['2019-01-01':'2022-01-01']\n",
    "\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "logging.info(f'Colunas Selecionadas: {colunas_selecionadas}')\n",
    "df.head(10)"
   ],
   "id": "d711b48520899f06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def impute_missing_values(df):\n",
    "    return df.interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "\n",
    "df_imputed = impute_missing_values(df)\n",
    "\n",
    "logging.info(f\"Dados ausentes antes da imputação: {df.isna().sum()}\")\n",
    "logging.info(f\"Dados ausentes após a imputação: {df_imputed.isna().sum()}\")"
   ],
   "id": "cf71a13126091c0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from copy import deepcopy as dc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# Preparando os dados para LSTM\n",
    "def prepare_dataframe_for_lstm(df, n_steps):\n",
    "    df = dc(df)\n",
    "    for col in colunas_selecionadas:\n",
    "        for i in range(1, n_steps + 1):\n",
    "            df[f'{col}(t-{i})'] = df[col].shift(i)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "lookback = 8  # 8 horas de lookback\n",
    "shifted_df = prepare_dataframe_for_lstm(df_imputed, lookback)\n",
    "\n",
    "# Normalizando os dados\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "shifted_df_as_np = scaler.fit_transform(shifted_df)\n",
    "\n",
    "X = shifted_df_as_np[:, len(colunas_selecionadas):]\n",
    "y = shifted_df_as_np[:, 0]  # Mantemos PM2.5 como nossa variável alvo\n",
    "\n",
    "X = dc(np.flip(X, axis=1))\n",
    "\n",
    "# Dividindo em conjuntos de treino, validação e teste\n",
    "train_split = int(len(X) * 0.7)\n",
    "val_split = int(len(X) * 0.85)\n",
    "\n",
    "X_train, X_val, X_test = X[:train_split], X[train_split:val_split], X[val_split:]\n",
    "y_train, y_val, y_test = y[:train_split], y[train_split:val_split], y[val_split:]"
   ],
   "id": "d5fef927653f6d40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TimeSeriesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y).unsqueeze(1)  # Add an extra dimension to match the model output\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ],
   "id": "c0b67d1bc9f8cf14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class LSTMModel(pl.LightningModule):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, learning_rate):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add a sequence length dimension\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.MSELoss()(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)"
   ],
   "id": "46bb2c33acbea483",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def objective(trial):\n",
    "    hidden_size = trial.suggest_int('hidden_size', 32, 256)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "\n",
    "    model = LSTMModel(\n",
    "        input_size=X.shape[1],\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        output_size=1,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "    train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "    val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "    checkpoint_callback = ModelCheckpoint(dirpath=f'../models/{data_hoje}', filename='best_model', save_top_k=1, monitor='val_loss', mode='min')\n",
    "    logger = TensorBoardLogger(f'../logs/{data_hoje}', name='lstm_optuna')\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=100,\n",
    "        callbacks=[early_stop_callback, checkpoint_callback],\n",
    "        logger=logger,\n",
    "        log_every_n_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    return trainer.callback_metrics['val_loss'].item()"
   ],
   "id": "e7adff7145697f85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "# Executar a otimização com Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "logging.info(f'Melhor trial: {study.best_trial.params}')\n",
    "\n",
    "# Treinar o modelo final com os melhores hiperparâmetros\n",
    "best_params = study.best_trial.params\n",
    "final_model = LSTMModel(\n",
    "    input_size=X.shape[1],\n",
    "    hidden_size=best_params['hidden_size'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    output_size=1,\n",
    "    learning_rate=best_params['learning_rate']\n",
    ")"
   ],
   "id": "cc68930685ece790",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=best_params['batch_size'])\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=best_params['batch_size'])\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=f'../models/{data_hoje}', filename='final_model', save_top_k=1, monitor='val_loss', mode='min')\n",
    "logger = TensorBoardLogger(f'../logs/{data_hoje}', name='lstm_final')"
   ],
   "id": "23e74679d14c4e62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=200,\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "trainer.fit(final_model, train_loader, val_loader)\n",
    "test_result = trainer.test(final_model, test_loader)\n",
    "\n",
    "logging.info(f'Resultado do teste: {test_result}')\n",
    "\n",
    "# Salvar o modelo final\n",
    "torch.save(final_model.state_dict(), f'../models/{data_hoje}/final_model.pth')\n",
    "\n",
    "fim_execucao = pd.Timestamp.now()\n",
    "logging.info(f'{fim_execucao} - Processo de treinamento do modelo LSTM concluído')\n",
    "logging.info(f'Tempo total de execução: {fim_execucao - inicio_execucao}')"
   ],
   "id": "335a2939826046d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d98bb6b6051a26d9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
