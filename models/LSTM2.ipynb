{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from copy import deepcopy as dc\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import optuna\n",
    "import torch.nn.functional as F\n",
    "import os"
   ],
   "id": "1ab0f1299a77df43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configuração inicial\n",
    "data_hoje = datetime.now().strftime('%d-%m')\n",
    "inicio_execucao = pd.Timestamp.now()\n",
    "\n",
    "# Criando diretórios para logs e plots\n",
    "os.makedirs(f'../logs/{data_hoje}', exist_ok=True)\n",
    "os.makedirs(f'../plots/{data_hoje}', exist_ok=True)\n",
    "\n",
    "# Configuração do logging\n",
    "logging.basicConfig(filename=f'../logs/{data_hoje}/lstm_optuna.log', level=logging.INFO, format='- %(message)s')\n",
    "logging.info('-' * 50)\n",
    "logging.info(f'{inicio_execucao} - Iniciando o processo de otimização e treinamento do modelo LSTM')\n",
    "\n",
    "# Carregando e preparando os dados\n",
    "df_original = pd.read_csv('../dados_tratados/combinado/Piratininga/Piratininga_tratado_combinado.csv',\n",
    "                          usecols=['PM2.5', 'Data e Hora', 'PM10', 'Monóxido de Carbono'], low_memory=False)\n",
    "\n",
    "df_original['Data e Hora'] = pd.to_datetime(df_original['Data e Hora'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_original.index = df_original['Data e Hora']\n",
    "df_original.sort_index(inplace=True)\n",
    "\n",
    "colunas_selecionadas = ['PM2.5', 'PM10', 'Monóxido de Carbono']\n",
    "df = df_original[colunas_selecionadas]\n",
    "df = df.loc['2019-01-01':'2022-01-01']\n",
    "\n",
    "df = df.apply(pd.to_numeric, errors='coerce')"
   ],
   "id": "418fe42641446a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def treat_outliers(df):\n",
    "    for column in df.columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "        df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "    return df\n",
    "\n",
    "\n",
    "# fazendo o logging de qual algoritmo de imputação foi utilizado\n",
    "def log_imputation(method_name, impute_function, df):\n",
    "    df_imputed = impute_function(df)\n",
    "    logging.info(f\"Imputação realizada usando: {method_name}\")\n",
    "    return df_imputed\n",
    "\n",
    "\n",
    "# Função para imputação de dados ausentes\n",
    "def backward_fill_imputer(df):\n",
    "    df_imputed = df.fillna(method='bfill')\n",
    "    return df_imputed\n",
    "\n",
    "\n",
    "def linear_interpolation_imputer(df):\n",
    "    df_imputed = df.interpolate(method='linear')\n",
    "    return df_imputed\n",
    "\n",
    "\n",
    "# df_imputed = log_imputation('Backward Fill', backward_fill_imputer, df)\n",
    "df_imputed = log_imputation('Linear Interpolation', linear_interpolation_imputer, df)\n",
    "\n",
    "logging.info(f\"Dados ausentes antes da imputação: {df.isna().sum()}\")\n",
    "logging.info(f\"Dados ausentes após a imputação: {df_imputed.isna().sum()}\")"
   ],
   "id": "65334a755512eced",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Preparando os dados para LSTM\n",
    "def prepare_dataframe_for_lstm(df, n_steps):\n",
    "    df = dc(df)\n",
    "    for col in colunas_selecionadas:\n",
    "        for i in range(1, n_steps + 1):\n",
    "            df[f'{col}(t-{i})'] = df[col].shift(i)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "lookback = 8\n",
    "shifted_df = prepare_dataframe_for_lstm(df_imputed, lookback)\n",
    "\n",
    "# Normalizando os dados\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = pd.DataFrame(scaler.fit_transform(shifted_df), columns=shifted_df.columns, index=shifted_df.index)\n",
    "\n",
    "X = scaled_data.iloc[:, len(colunas_selecionadas):].values\n",
    "y = scaled_data.iloc[:, 0].values  # PM2.5 como variável alvo\n",
    "\n",
    "# Dividindo em conjuntos de treino, validação e teste\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.15)\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]\n",
    "X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]\n",
    "\n",
    "# Reshape para LSTM\n",
    "X_train = X_train.reshape((-1, lookback, len(colunas_selecionadas)))\n",
    "X_val = X_val.reshape((-1, lookback, len(colunas_selecionadas)))\n",
    "X_test = X_test.reshape((-1, lookback, len(colunas_selecionadas)))\n",
    "y_train = y_train.reshape((-1, 1))\n",
    "y_val = y_val.reshape((-1, 1))\n",
    "y_test = y_test.reshape((-1, 1))\n",
    "\n",
    "# Convertendo para tensores PyTorch\n",
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).float()\n",
    "X_val = torch.tensor(X_val).float()\n",
    "y_val = torch.tensor(y_val).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).float()\n",
    "\n",
    "\n",
    "# Dataset e DataLoader\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "\n",
    "# Modelo LSTM\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ],
   "id": "dd40f0a27c8b6d44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, activation, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.num_layers = len(hidden_sizes)\n",
    "        self.activation = activation\n",
    "\n",
    "        self.lstm_layers = nn.ModuleList([\n",
    "            nn.LSTM(input_size if i == 0 else hidden_sizes[i - 1],\n",
    "                    hidden_sizes[i],\n",
    "                    num_layers=1,\n",
    "                    batch_first=True)\n",
    "            for i in range(len(hidden_sizes))\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) if self.num_layers > 1 else nn.Identity()\n",
    "        self.fc = nn.Linear(hidden_sizes[-1], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        for i, lstm in enumerate(self.lstm_layers):\n",
    "            h0 = torch.zeros(1, batch_size, self.hidden_sizes[i]).to(x.device)\n",
    "            c0 = torch.zeros(1, batch_size, self.hidden_sizes[i]).to(x.device)\n",
    "            x, _ = lstm(x, (h0, c0))\n",
    "\n",
    "            if i < len(self.lstm_layers) - 1:\n",
    "                x = self.apply_activation(x)\n",
    "                x = self.dropout(x)\n",
    "\n",
    "        out = self.fc(x[:, -1, :])\n",
    "        return out\n",
    "\n",
    "    def apply_activation(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            return F.relu(x)\n",
    "        elif self.activation == 'tanh':\n",
    "            return F.tanh(x)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return F.sigmoid(x)\n",
    "        elif self.activation == 'leaky_relu':\n",
    "            return F.leaky_relu(x)\n",
    "        elif self.activation == 'elu':\n",
    "            return F.elu(x)\n",
    "        else:\n",
    "            return x"
   ],
   "id": "8d8cbc463f878c02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Função objetivo para Optuna\n",
    "def objective(trial):\n",
    "    # Hiperparâmetros para otimização\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    hidden_sizes = [trial.suggest_int(f'hidden_size_{i}', 32, 256) for i in range(num_layers)]\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'leaky_relu', 'elu'])\n",
    "    dropout = trial.suggest_float('dropout', 0.0, 0.5)\n",
    "\n",
    "    # Criação dos DataLoaders\n",
    "    train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "    val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Criação e treinamento do modelo\n",
    "    model = LSTM(len(colunas_selecionadas), hidden_sizes, activation, dropout).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.HuberLoss()\n",
    "\n",
    "    num_epochs = 1000\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_patience = 50\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "                output = model(x_batch)\n",
    "                val_loss += criterion(output, y_batch).item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= early_stopping_patience:\n",
    "            break\n",
    "\n",
    "    return best_val_loss"
   ],
   "id": "5b1ce00606d3df1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Execução da otimização\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_params = study.best_params\n",
    "logging.info(f\"Melhores hiperparâmetros: {best_params}\")\n",
    "\n",
    "best_hidden_sizes = [best_params[f'hidden_size_{i}'] for i in range(best_params['num_layers'])]\n",
    "best_batch_size = best_params['batch_size']\n",
    "best_learning_rate = best_params['learning_rate']\n",
    "best_activation = best_params['activation']\n",
    "best_dropout = best_params['dropout']\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "\n",
    "final_model = LSTM(len(colunas_selecionadas), best_hidden_sizes, best_activation, best_dropout).to(device)\n",
    "optimizer = torch.optim.AdamW(final_model.parameters(), lr=best_learning_rate)\n",
    "criterion = nn.HuberLoss()\n",
    "\n",
    "# Treinamento final\n",
    "num_epochs = 1000\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_patience = 50\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    final_model.train()\n",
    "    for batch in train_loader:\n",
    "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = final_model(x_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    final_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "            output = final_model(x_batch)\n",
    "            val_loss += criterion(output, y_batch).item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    logging.info(f\"Época {epoch + 1}/{num_epochs}, Perda de validação: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(final_model.state_dict(), f'../models/best_model_optuna_{data_hoje}.pth')\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= early_stopping_patience:\n",
    "        logging.info(f\"Early stopping ativado na época {epoch + 1}\")\n",
    "        break\n",
    "\n",
    "# Avaliação final\n",
    "final_model.load_state_dict(torch.load(f'../models/best_model_optuna_{data_hoje}.pth'))\n",
    "final_model.eval()\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    predictions = []\n",
    "    actual = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
    "            output = model(x_batch)\n",
    "            predictions.extend(output.cpu().numpy().flatten())\n",
    "            actual.extend(y_batch.cpu().numpy().flatten())\n",
    "    return np.array(predictions), np.array(actual)\n",
    "\n",
    "\n",
    "train_predictions, train_actual = evaluate(final_model, train_loader)\n",
    "val_predictions, val_actual = evaluate(final_model, val_loader)\n",
    "test_predictions, test_actual = evaluate(final_model, test_loader)"
   ],
   "id": "f608d404a5cd2463",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Desnormalização\n",
    "def inverse_transform_data(normalized_data):\n",
    "    # Criar um array de zeros com o mesmo número de colunas que os dados originais\n",
    "    dummy_array = np.zeros((len(normalized_data), len(shifted_df.columns)))\n",
    "    \n",
    "    # Colocar os dados normalizados na primeira coluna (assumindo que é PM2.5)\n",
    "    dummy_array[:, 0] = normalized_data\n",
    "    \n",
    "    # Aplicar a transformação inversa\n",
    "    denormalized_data = scaler.inverse_transform(dummy_array)\n",
    "    \n",
    "    # Retornar apenas a primeira coluna, que contém os dados desnormalizados de interesse\n",
    "    return denormalized_data[:, 0]\n",
    "\n",
    "# Desnormalização das previsões e valores reais\n",
    "train_predictions = inverse_transform_data(train_predictions)\n",
    "val_predictions = inverse_transform_data(val_predictions)\n",
    "test_predictions = inverse_transform_data(test_predictions)\n",
    "\n",
    "# Para os valores reais, precisamos garantir que estamos usando os dados originais não normalizados\n",
    "train_actual = df_imputed['PM2.5'].values[:len(train_predictions)]\n",
    "val_actual = df_imputed['PM2.5'].values[len(train_predictions):len(train_predictions)+len(val_predictions)]\n",
    "test_actual = df_imputed['PM2.5'].values[-len(test_predictions):]\n",
    "\n",
    "# Cálculo das métricas\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return rmse, mse, mae, r2\n",
    "\n",
    "# Calcular métricas\n",
    "train_rmse, train_mse, train_mae, train_r2 = calculate_metrics(train_actual, train_predictions)\n",
    "val_rmse, val_mse, val_mae, val_r2 = calculate_metrics(val_actual, val_predictions)\n",
    "test_rmse, test_mse, test_mae, test_r2 = calculate_metrics(test_actual, test_predictions)\n",
    "\n",
    "# Logging dos resultados\n",
    "logging.info(f\"Métricas de Treino: RMSE={train_rmse:.4f}, MSE={train_mse:.4f}, MAE={train_mae:.4f}, R2={train_r2:.4f}\")\n",
    "logging.info(f\"Métricas de Validação: RMSE={val_rmse:.4f}, MSE={val_mse:.4f}, MAE={val_mae:.4f}, R2={val_r2:.4f}\")\n",
    "logging.info(f\"Métricas de Teste: RMSE={test_rmse:.4f}, MSE={test_mse:.4f}, MAE={test_mae:.4f}, R2={test_r2:.4f}\")"
   ],
   "id": "8ed03d304545ce81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Log das métricas finais\n",
    "logging.info(\"\\nMétricas finais:\")\n",
    "logging.info(\"Treinamento - RMSE: {:.4f}, MSE: {:.4f}, MAE: {:.4f}, R²: {:.4f}\".format(train_rmse, train_mse, train_mae,\n",
    "                                                                                       train_r2))\n",
    "logging.info(\n",
    "    \"Validação - RMSE: {:.4f}, MSE: {:.4f}, MAE: {:.4f}, R²: {:.4f}\".format(val_rmse, val_mse, val_mae, val_r2))\n",
    "logging.info(\n",
    "    \"Teste - RMSE: {:.4f}, MSE: {:.4f}, MAE: {:.4f}, R²: {:.4f}\".format(test_rmse, test_mse, test_mae, test_r2))\n",
    "\n",
    "# Plotagem dos resultados\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_actual, label='Actual PM2.5')\n",
    "plt.plot(train_predictions, label='Predicted PM2.5')\n",
    "plt.title('Treinamento: PM2.5 Real vs Previsto')\n",
    "plt.xlabel('Hora')\n",
    "plt.ylabel('PM2.5')\n",
    "plt.legend()\n",
    "plt.savefig(f'../plots/{data_hoje}/lstm_optuna_train_{data_hoje}.png')\n",
    "plt.close()"
   ],
   "id": "60faf0cdcf264037",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "train_dates = shifted_df.index[:len(train_actual)]\n",
    "val_dates = shifted_df.index[len(train_actual):len(train_actual) + len(val_actual)]\n",
    "test_dates = shifted_df.index[-len(test_actual):]\n",
    "\n",
    "\n",
    "def plot_results(actual, predicted, dates, title, filename):\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    plt.plot(dates, actual, label='Real', color='blue')\n",
    "    plt.plot(dates, predicted, label='Previsto', color='red')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Data')\n",
    "    plt.ylabel('PM2.5')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))  # Mostrar a cada 3 meses\n",
    "\n",
    "    plt.gcf().autofmt_xdate()  # Rotacionar e alinhar os rótulos de data\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../plots/{data_hoje}/{filename}_{data_hoje}.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "plot_results(train_actual, train_predictions, train_dates, 'Treinamento: PM2.5 Real vs Previsto', 'lstm_optuna_train')\n",
    "plot_results(val_actual, val_predictions, val_dates, 'Validação: PM2.5 Real vs Previsto', 'lstm_optuna_val')\n",
    "plot_results(test_actual, test_predictions, test_dates, 'Teste: PM2.5 Real vs Previsto', 'lstm_optuna_test')\n",
    "\n",
    "fim_execucao = pd.Timestamp.now()\n",
    "tempo_execucao = fim_execucao - inicio_execucao\n",
    "logging.info(f\"\\nExecução finalizada em {fim_execucao}\")\n",
    "logging.info(f\"Tempo total de execução: {tempo_execucao}\")"
   ],
   "id": "19edab1f2a08d516",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def plot_results_by_month(actual, predicted, dates, title_prefix, filename_prefix):\n",
    "    df = pd.DataFrame({'date': dates, 'actual': actual, 'predicted': predicted})\n",
    "    df.set_index('date', inplace=True)\n",
    "\n",
    "    grouped = df.groupby(pd.Grouper(freq='M'))\n",
    "\n",
    "    for name, group in grouped:\n",
    "        if len(group) > 0:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(group.index, group['actual'], label='Real', color='blue')\n",
    "            plt.plot(group.index, group['predicted'], label='Previsto', color='red')\n",
    "\n",
    "            month_year = name.strftime('%B %Y')\n",
    "            plt.title(f'{title_prefix} - {month_year}')\n",
    "            plt.xlabel('Data')\n",
    "            plt.ylabel('PM2.5')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d-%m'))\n",
    "            plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=5))\n",
    "\n",
    "            plt.gcf().autofmt_xdate()  # Rotacionar e alinhar os rótulos de data\n",
    "            plt.tight_layout()\n",
    "\n",
    "            month_filename = f'{filename_prefix}_{name.strftime(\"%Y_%m\")}_{data_hoje}.png'\n",
    "            plt.savefig(f'../plots/{data_hoje}/{month_filename}')\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "plot_results_by_month(train_actual, train_predictions, train_dates, 'Treinamento: PM2.5 Real vs Previsto',\n",
    "                      'lstm_optuna_train')\n",
    "plot_results_by_month(val_actual, val_predictions, val_dates, 'Validação: PM2.5 Real vs Previsto', 'lstm_optuna_val')\n",
    "plot_results_by_month(test_actual, test_predictions, test_dates, 'Teste: PM2.5 Real vs Previsto', 'lstm_optuna_test')"
   ],
   "id": "5d311845f2a60318",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
