{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ],
   "id": "d14921d7ecd62577",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using {device}')\n",
    "\n",
    "df = pd.read_csv('../dados_tratados/combinado/Piratininga/Piratininga_tratado_combinado.csv',\n",
    "                 usecols=['PM2.5', 'Data e Hora'])\n",
    "df.dropna(inplace=True)\n",
    "df.index = pd.to_datetime(df['Data e Hora'], format='%Y-%m-%d %H:%M:%S')\n",
    "train_dates = pd.to_datetime(df['Data e Hora'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df['Data e Hora'] = pd.to_datetime(df['Data e Hora'])\n",
    "df['hour'] = df['Data e Hora'].dt.hour\n",
    "df['minute'] = df['Data e Hora'].dt.minute\n",
    "df['year'] = df['Data e Hora'].dt.year\n",
    "df['month'] = df['Data e Hora'].dt.month\n",
    "df['day'] = df['Data e Hora'].dt.day\n",
    "df['day_of_week'] = df['Data e Hora'].dt.dayofweek\n",
    "df['day_of_year'] = df['Data e Hora'].dt.dayofyear\n",
    "df['week'] = df['Data e Hora'].dt.isocalendar().week\n",
    "\n",
    "df.drop('Data e Hora', axis=1, inplace=True)\n",
    "\n"
   ],
   "id": "fda3343bbafcd43e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Normalizando os dados de PM2.5\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_values = scaler.fit_transform(df['PM2.5'].values.reshape(-1, 1))\n",
    "\n",
    "df['PM2.5'] = scaled_values\n",
    "\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data) - seq_length - 1):\n",
    "        x = data[i:(i + seq_length), :].astype(np.float32)\n",
    "        y = data[i + seq_length, 0]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "\n",
    "seq_length = 23\n",
    "X, y = create_sequences(df.values, seq_length)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float().reshape(-1, 1)\n",
    "X_val = torch.from_numpy(X_val).float()\n",
    "y_val = torch.from_numpy(y_val).float().reshape(-1, 1)\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float().reshape(-1, 1)\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ],
   "id": "4011bc3253703272",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, num_layers, output_size, drop_prob=0.2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, _ = self.lstm(input_seq)\n",
    "        predictions = self.linear(lstm_out[:, -1, :])\n",
    "        return predictions\n",
    "\n",
    "\n",
    "input_size = X_train.shape[2]\n",
    "model = LSTM(input_size=input_size, hidden_layer_size=512, num_layers=2, output_size=1).to(device)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_function, optimizer, epochs=150, patience=10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = np.inf\n",
    "    counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        # Loop de treinamento\n",
    "        for seq, labels in train_loader:\n",
    "            seq, labels = seq.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(seq)\n",
    "            single_loss = loss_function(y_pred, labels)\n",
    "            single_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += single_loss.item() * seq.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        # Loop de validação\n",
    "        with torch.no_grad():\n",
    "            for seq, labels in val_loader:\n",
    "                seq, labels = seq.to(device), labels.to(device)\n",
    "                y_pred = model(seq)\n",
    "                single_loss = loss_function(y_pred, labels)\n",
    "                val_loss += single_loss.item() * seq.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch} train loss: {train_loss}, val loss: {val_loss}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader, loss_function, optimizer, epochs=200,\n",
    "                                       patience=10)\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq, labels in test_loader:\n",
    "        seq, labels = seq.to(device), labels.to(device)\n",
    "        y_pred = model(seq)\n",
    "        test_predictions.append(y_pred.cpu().numpy())\n",
    "\n",
    "test_predictions = np.concatenate(test_predictions, axis=0)\n",
    "test_predictions = scaler.inverse_transform(test_predictions.reshape(-1, 1))\n",
    "y_test = scaler.inverse_transform(y_test.cpu().numpy().reshape(-1, 1))\n",
    "\n",
    "mae = np.mean(np.abs(test_predictions - y_test))\n",
    "mse = np.mean((test_predictions - y_test) ** 2)\n",
    "mape = np.mean(np.abs(test_predictions - y_test) / np.abs(y_test)) * 100\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Metrics:\")\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'Root Mean Squared Error: {rmse}')\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Mean Absolute Percentage Error: {mape}')\n",
    "\n",
    "# Plotagem das perdas de treinamento e validação\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "cd670cddfc773300",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-07T17:25:20.290542Z",
     "start_time": "2024-07-07T16:20:25.312334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# Definição do modelo LSTM\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, num_layers, output_size, drop_prob, activation_function):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, _ = self.lstm(input_seq)\n",
    "        predictions = self.linear(lstm_out[:, -1, :])\n",
    "        if self.activation_function == 'relu':\n",
    "            predictions = nn.ReLU()(predictions)\n",
    "        elif self.activation_function == 'tanh':\n",
    "            predictions = nn.Tanh()(predictions)\n",
    "        elif self.activation_function == 'sigmoid':\n",
    "            predictions = nn.Sigmoid()(predictions)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Função de treinamento e avaliação do modelo\n",
    "def train_and_evaluate_lstm(hidden_layer_size, num_layers, lr, batch_size, drop_prob, activation_function, weight_decay,\n",
    "                            num_epochs, patience):\n",
    "    model = LSTM(\n",
    "        input_size=input_size,\n",
    "        hidden_layer_size=hidden_layer_size,\n",
    "        num_layers=num_layers,\n",
    "        drop_prob=drop_prob,\n",
    "        output_size=1,\n",
    "        activation_function=activation_function\n",
    "    ).to(device)\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    train_data = TensorDataset(X_train, y_train)\n",
    "    val_data = TensorDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Early stopping \n",
    "    best_val_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    def train_model():\n",
    "        model.train()\n",
    "        for seq, labels in train_loader:\n",
    "            seq, labels = seq.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(seq)\n",
    "            single_loss = loss_function(y_pred, labels)\n",
    "            single_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def evaluate_model():\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for seq, labels in val_loader:\n",
    "                seq, labels = seq.to(device), labels.to(device)\n",
    "                y_pred = model(seq)\n",
    "                single_loss = loss_function(y_pred, labels)\n",
    "                val_loss += single_loss.item() * seq.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        return val_loss\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_model()\n",
    "        val_loss = evaluate_model()\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "    \n",
    "    torch.save(model.state_dict(), '../best_model.pth')\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class PyTorchLSTMRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, hidden_layer_size=100, num_layers=2, lr=0.001, batch_size=64, drop_prob=0.2,\n",
    "                 activation_function='relu', weight_decay=1e-8, num_epochs=50, patience=5):\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lr = lr\n",
    "        self.drop_prob = drop_prob\n",
    "        self.batch_size = batch_size\n",
    "        self.activation_function = activation_function\n",
    "        self.weight_decay = weight_decay\n",
    "        self.num_epochs = num_epochs\n",
    "        self.patience = patience\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model = LSTM(input_size=input_size, hidden_layer_size=self.hidden_layer_size, num_layers=self.num_layers,\n",
    "                          drop_prob=self.drop_prob, output_size=1, activation_function=self.activation_function).to(\n",
    "            device)\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        train_data = TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(y).float().reshape(-1, 1))\n",
    "        self.train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.train_model()\n",
    "        return self\n",
    "\n",
    "    def train_model(self):\n",
    "        self.model.train()\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for seq, labels in self.train_loader:\n",
    "                seq, labels = seq.to(device), labels.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                y_pred = self.model(seq)\n",
    "                single_loss = self.loss_function(y_pred, labels)\n",
    "                single_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        test_data = torch.from_numpy(X).float()\n",
    "        test_loader = DataLoader(test_data, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for seq in test_loader:\n",
    "                seq = seq.to(device)\n",
    "                y_pred = self.model(seq)\n",
    "                predictions.append(y_pred.cpu().numpy())\n",
    "\n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Parâmetros para busca\n",
    "param_space = {\n",
    "    'hidden_layer_size': Integer(64, 512),\n",
    "    'num_layers': Integer(1, 5),\n",
    "    'lr': Real(0.0001, 0.01, prior='log-uniform'),\n",
    "    'batch_size': Categorical([32, 64, 96, 128]),\n",
    "    'num_epochs': Categorical([100, 200, 300, 400]),\n",
    "    'activation_function': Categorical(['relu', 'tanh', 'sigmoid']),\n",
    "    'drop_prob': Categorical([0.1, 0.2, 0.3]),\n",
    "    'weight_decay': Categorical([1e-6, 1e-4, 0.01]),\n",
    "    'patience': Categorical([3, 10, 15])\n",
    "}\n",
    "\n",
    "# Usando BayesSearchCV\n",
    "bayes_search = BayesSearchCV(estimator=PyTorchLSTMRegressor(), search_spaces=param_space,\n",
    "                             scoring='neg_mean_squared_error', cv=3, n_iter=30, random_state=42, verbose=3)\n",
    "bayes_search.fit(X_train.numpy(), y_train.numpy())\n",
    "\n",
    "print(\"Best Parameters:\", bayes_search.best_params_)\n",
    "print(\"Best Score:\", -bayes_search.best_score_)\n"
   ],
   "id": "149c4f7e7d058a8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV 1/3] END activation_function=tanh, batch_size=96, drop_prob=0.3, hidden_layer_size=205, lr=0.0021892527128216433, num_epochs=50, num_layers=2, patience=12, weight_decay=1e-06;, score=-0.003 total time= 1.4min\n",
      "[CV 2/3] END activation_function=tanh, batch_size=96, drop_prob=0.3, hidden_layer_size=205, lr=0.0021892527128216433, num_epochs=50, num_layers=2, patience=12, weight_decay=1e-06;, score=-0.003 total time= 1.4min\n",
      "[CV 3/3] END activation_function=tanh, batch_size=96, drop_prob=0.3, hidden_layer_size=205, lr=0.0021892527128216433, num_epochs=50, num_layers=2, patience=12, weight_decay=1e-06;, score=-0.003 total time= 1.4min\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV 1/3] END activation_function=sigmoid, batch_size=128, drop_prob=0.1, hidden_layer_size=490, lr=0.005348793493196708, num_epochs=30, num_layers=2, patience=6, weight_decay=0.0001;, score=-0.007 total time=  51.0s\n",
      "[CV 2/3] END activation_function=sigmoid, batch_size=128, drop_prob=0.1, hidden_layer_size=490, lr=0.005348793493196708, num_epochs=30, num_layers=2, patience=6, weight_decay=0.0001;, score=-0.003 total time=  51.4s\n",
      "[CV 3/3] END activation_function=sigmoid, batch_size=128, drop_prob=0.1, hidden_layer_size=490, lr=0.005348793493196708, num_epochs=30, num_layers=2, patience=6, weight_decay=0.0001;, score=-0.003 total time=  51.0s\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV 1/3] END activation_function=tanh, batch_size=128, drop_prob=0.1, hidden_layer_size=258, lr=0.00023763498182367168, num_epochs=50, num_layers=2, patience=12, weight_decay=0.0001;, score=-0.003 total time= 1.1min\n",
      "[CV 2/3] END activation_function=tanh, batch_size=128, drop_prob=0.1, hidden_layer_size=258, lr=0.00023763498182367168, num_epochs=50, num_layers=2, patience=12, weight_decay=0.0001;, score=-0.002 total time= 1.1min\n",
      "[CV 3/3] END activation_function=tanh, batch_size=128, drop_prob=0.1, hidden_layer_size=258, lr=0.00023763498182367168, num_epochs=50, num_layers=2, patience=12, weight_decay=0.0001;, score=-0.002 total time= 1.1min\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV 1/3] END activation_function=sigmoid, batch_size=32, drop_prob=0.2, hidden_layer_size=424, lr=0.0011119996642263944, num_epochs=30, num_layers=4, patience=15, weight_decay=0.01;, score=-0.003 total time= 4.2min\n",
      "[CV 2/3] END activation_function=sigmoid, batch_size=32, drop_prob=0.2, hidden_layer_size=424, lr=0.0011119996642263944, num_epochs=30, num_layers=4, patience=15, weight_decay=0.01;, score=-0.006 total time= 4.2min\n",
      "[CV 3/3] END activation_function=sigmoid, batch_size=32, drop_prob=0.2, hidden_layer_size=424, lr=0.0011119996642263944, num_epochs=30, num_layers=4, patience=15, weight_decay=0.01;, score=-0.003 total time= 4.2min\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV 1/3] END activation_function=sigmoid, batch_size=64, drop_prob=0.2, hidden_layer_size=385, lr=0.006422500474336274, num_epochs=70, num_layers=3, patience=12, weight_decay=1e-06;, score=-0.007 total time= 4.0min\n",
      "[CV 2/3] END activation_function=sigmoid, batch_size=64, drop_prob=0.2, hidden_layer_size=385, lr=0.006422500474336274, num_epochs=70, num_layers=3, patience=12, weight_decay=1e-06;, score=-0.007 total time= 3.9min\n",
      "[CV 3/3] END activation_function=sigmoid, batch_size=64, drop_prob=0.2, hidden_layer_size=385, lr=0.006422500474336274, num_epochs=70, num_layers=3, patience=12, weight_decay=1e-06;, score=-0.007 total time= 4.0min\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV 1/3] END activation_function=sigmoid, batch_size=128, drop_prob=0.1, hidden_layer_size=148, lr=0.00399030840458803, num_epochs=50, num_layers=3, patience=9, weight_decay=0.01;, score=-0.004 total time= 2.3min\n",
      "[CV 2/3] END activation_function=sigmoid, batch_size=128, drop_prob=0.1, hidden_layer_size=148, lr=0.00399030840458803, num_epochs=50, num_layers=3, patience=9, weight_decay=0.01;, score=-0.003 total time= 2.3min\n",
      "[CV 3/3] END activation_function=sigmoid, batch_size=128, drop_prob=0.1, hidden_layer_size=148, lr=0.00399030840458803, num_epochs=50, num_layers=3, patience=9, weight_decay=0.01;, score=-0.003 total time= 2.3min\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV 1/3] END activation_function=tanh, batch_size=128, drop_prob=0.2, hidden_layer_size=456, lr=0.001531931515055945, num_epochs=70, num_layers=4, patience=9, weight_decay=0.0001;, score=-0.003 total time= 3.1min\n",
      "[CV 2/3] END activation_function=tanh, batch_size=128, drop_prob=0.2, hidden_layer_size=456, lr=0.001531931515055945, num_epochs=70, num_layers=4, patience=9, weight_decay=0.0001;, score=-0.003 total time= 3.2min\n",
      "[CV 3/3] END activation_function=tanh, batch_size=128, drop_prob=0.2, hidden_layer_size=456, lr=0.001531931515055945, num_epochs=70, num_layers=4, patience=9, weight_decay=0.0001;, score=-0.003 total time= 3.2min\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV 1/3] END activation_function=tanh, batch_size=128, drop_prob=0.2, hidden_layer_size=439, lr=0.00044475410431869377, num_epochs=30, num_layers=3, patience=3, weight_decay=1e-06;, score=-0.003 total time= 1.1min\n",
      "[CV 2/3] END activation_function=tanh, batch_size=128, drop_prob=0.2, hidden_layer_size=439, lr=0.00044475410431869377, num_epochs=30, num_layers=3, patience=3, weight_decay=1e-06;, score=-0.002 total time= 1.1min\n",
      "[CV 3/3] END activation_function=tanh, batch_size=128, drop_prob=0.2, hidden_layer_size=439, lr=0.00044475410431869377, num_epochs=30, num_layers=3, patience=3, weight_decay=1e-06;, score=-0.003 total time= 1.1min\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV 1/3] END activation_function=sigmoid, batch_size=96, drop_prob=0.3, hidden_layer_size=250, lr=0.0005782684919514924, num_epochs=30, num_layers=1, patience=6, weight_decay=0.0001;, score=-0.003 total time=  37.5s\n",
      "[CV 2/3] END activation_function=sigmoid, batch_size=96, drop_prob=0.3, hidden_layer_size=250, lr=0.0005782684919514924, num_epochs=30, num_layers=1, patience=6, weight_decay=0.0001;, score=-0.002 total time=  37.4s\n",
      "[CV 3/3] END activation_function=sigmoid, batch_size=96, drop_prob=0.3, hidden_layer_size=250, lr=0.0005782684919514924, num_epochs=30, num_layers=1, patience=6, weight_decay=0.0001;, score=-0.002 total time=  37.5s\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV 1/3] END activation_function=relu, batch_size=128, drop_prob=0.3, hidden_layer_size=178, lr=0.0014365762176299867, num_epochs=50, num_layers=2, patience=3, weight_decay=0.01;, score=-0.003 total time= 2.2min\n",
      "[CV 2/3] END activation_function=relu, batch_size=128, drop_prob=0.3, hidden_layer_size=178, lr=0.0014365762176299867, num_epochs=50, num_layers=2, patience=3, weight_decay=0.01;, score=-0.003 total time= 2.2min\n",
      "[CV 3/3] END activation_function=relu, batch_size=128, drop_prob=0.3, hidden_layer_size=178, lr=0.0014365762176299867, num_epochs=50, num_layers=2, patience=3, weight_decay=0.01;, score=-0.003 total time= 2.2min\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV 1/3] END activation_function=relu, batch_size=128, drop_prob=0.3, hidden_layer_size=259, lr=0.0001955692971851212, num_epochs=70, num_layers=1, patience=15, weight_decay=1e-08;, score=-0.002 total time= 1.2min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 167\u001B[0m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;66;03m# Usando BayesSearchCV\u001B[39;00m\n\u001B[0;32m    165\u001B[0m bayes_search \u001B[38;5;241m=\u001B[39m BayesSearchCV(estimator\u001B[38;5;241m=\u001B[39mPyTorchLSTMRegressor(), search_spaces\u001B[38;5;241m=\u001B[39mparam_space,\n\u001B[0;32m    166\u001B[0m                              scoring\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mneg_mean_squared_error\u001B[39m\u001B[38;5;124m'\u001B[39m, cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, n_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m--> 167\u001B[0m \u001B[43mbayes_search\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest Parameters:\u001B[39m\u001B[38;5;124m\"\u001B[39m, bayes_search\u001B[38;5;241m.\u001B[39mbest_params_)\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest Score:\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m-\u001B[39mbayes_search\u001B[38;5;241m.\u001B[39mbest_score_)\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\skopt\\searchcv.py:542\u001B[0m, in \u001B[0;36mBayesSearchCV.fit\u001B[1;34m(self, X, y, groups, callback, **fit_params)\u001B[0m\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrefit):\n\u001B[0;32m    536\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    537\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBayesSearchCV doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt support a callable refit, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    538\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt define an implicit score to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    539\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moptimize\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    540\u001B[0m     )\n\u001B[1;32m--> 542\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    544\u001B[0m \u001B[38;5;66;03m# BaseSearchCV never ranked train scores,\u001B[39;00m\n\u001B[0;32m    545\u001B[0m \u001B[38;5;66;03m# but apparently we used to ship this (back-compat)\u001B[39;00m\n\u001B[0;32m    546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_train_score:\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m   1012\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[0;32m   1013\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[0;32m   1014\u001B[0m     )\n\u001B[0;32m   1016\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m-> 1018\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1020\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[0;32m   1021\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[0;32m   1022\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\skopt\\searchcv.py:599\u001B[0m, in \u001B[0;36mBayesSearchCV._run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m    595\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m n_iter \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    596\u001B[0m     \u001B[38;5;66;03m# when n_iter < n_points points left for evaluation\u001B[39;00m\n\u001B[0;32m    597\u001B[0m     n_points_adjusted \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(n_iter, n_points)\n\u001B[1;32m--> 599\u001B[0m     optim_result, score_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    600\u001B[0m \u001B[43m        \u001B[49m\u001B[43msearch_space\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    601\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    602\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscore_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    603\u001B[0m \u001B[43m        \u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    604\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_points\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_points_adjusted\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    605\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    606\u001B[0m     n_iter \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m n_points\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m eval_callbacks(callbacks, optim_result):\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\skopt\\searchcv.py:453\u001B[0m, in \u001B[0;36mBayesSearchCV._step\u001B[1;34m(self, search_space, optimizer, score_name, evaluate_candidates, n_points)\u001B[0m\n\u001B[0;32m    450\u001B[0m \u001B[38;5;66;03m# make lists into dictionaries\u001B[39;00m\n\u001B[0;32m    451\u001B[0m params_dict \u001B[38;5;241m=\u001B[39m [point_asdict(search_space, p) \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m params]\n\u001B[1;32m--> 453\u001B[0m all_results \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;66;03m# if self.scoring is a callable, we have to wait until here\u001B[39;00m\n\u001B[0;32m    456\u001B[0m \u001B[38;5;66;03m# to get the score name\u001B[39;00m\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m score_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[1;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[0;32m    956\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    957\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    958\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    959\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    960\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[0;32m    961\u001B[0m         )\n\u001B[0;32m    962\u001B[0m     )\n\u001B[1;32m--> 964\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    965\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    966\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    967\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    968\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    969\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    970\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    971\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    972\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    973\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    974\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    975\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    976\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    977\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    978\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplitter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    979\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    980\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    982\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    983\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    984\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    985\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    986\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    987\u001B[0m     )\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     69\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     70\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     71\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     73\u001B[0m )\n\u001B[1;32m---> 74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\joblib\\parallel.py:1918\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1916\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[0;32m   1917\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n\u001B[0;32m   1920\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[0;32m   1921\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[0;32m   1922\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[0;32m   1923\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[0;32m   1924\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[0;32m   1925\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\joblib\\parallel.py:1847\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1845\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1846\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1847\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1848\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1849\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    134\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    135\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m--> 136\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:888\u001B[0m, in \u001B[0;36m_fit_and_score\u001B[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[0m\n\u001B[0;32m    886\u001B[0m         estimator\u001B[38;5;241m.\u001B[39mfit(X_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m    887\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 888\u001B[0m         \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    890\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    891\u001B[0m     \u001B[38;5;66;03m# Note fit time as time until error\u001B[39;00m\n\u001B[0;32m    892\u001B[0m     fit_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time\n",
      "Cell \u001B[1;32mIn[13], line 121\u001B[0m, in \u001B[0;36mPyTorchLSTMRegressor.fit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    118\u001B[0m train_data \u001B[38;5;241m=\u001B[39m TensorDataset(torch\u001B[38;5;241m.\u001B[39mfrom_numpy(X)\u001B[38;5;241m.\u001B[39mfloat(), torch\u001B[38;5;241m.\u001B[39mfrom_numpy(y)\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_loader \u001B[38;5;241m=\u001B[39m DataLoader(train_data, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m--> 121\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "Cell \u001B[1;32mIn[13], line 127\u001B[0m, in \u001B[0;36mPyTorchLSTMRegressor.train_model\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    125\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_epochs):\n\u001B[1;32m--> 127\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mseq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m    128\u001B[0m \u001B[43m        \u001B[49m\u001B[43mseq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mseq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    129\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzero_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:316\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    256\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    257\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[0;32m    258\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    314\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    315\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    170\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m[\u001B[49m\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msamples\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtransposed\u001B[49m\u001B[43m]\u001B[49m  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:173\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    170\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    175\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 141\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    143\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    144\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32mC:\\dev\\scripts\\pm25-plots\\venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:213\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    211\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    212\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[1;32m--> 213\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "best_model = PyTorchLSTMRegressor(**bayes_search.best_params_)\n",
    "best_model.fit(X_train.numpy(), y_train.numpy())\n",
    "torch.save(best_model.model.state_dict(), '../best_model.pth')"
   ],
   "id": "b8a31fb7966ffc50"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
