{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T20:43:36.076218Z",
     "start_time": "2024-07-04T20:43:22.844414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "df = pd.read_csv('./dados_tratados/combinado/Piratininga/Piratininga_tratado_combinado.csv',\n",
    "                 usecols=['PM2.5', 'Data e Hora'])\n",
    "\n",
    "\n",
    "df['Data e Hora'] = pd.to_datetime(df['Data e Hora'])\n",
    "df['hour'] = df['Data e Hora'].dt.hour\n",
    "df['minute'] = df['Data e Hora'].dt.minute\n",
    "df['year'] = df['Data e Hora'].dt.year\n",
    "df['month'] = df['Data e Hora'].dt.month\n",
    "df['day'] = df['Data e Hora'].dt.day\n",
    "df['day_of_week'] = df['Data e Hora'].dt.dayofweek\n",
    "df['day_of_year'] = df['Data e Hora'].dt.dayofyear\n",
    "df['week'] = df['Data e Hora'].dt.isocalendar().week\n",
    "\n",
    "df.drop('Data e Hora', axis=1, inplace=True)\n",
    "\n",
    "# Normalizando os dados de PM2.5\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_values = scaler.fit_transform(df[['PM2.5']].values)\n",
    "\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data) - seq_length - 1):\n",
    "        x = data[i:(i + seq_length)]\n",
    "        y = data[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "\n",
    "seq_length = 8\n",
    "X, y = create_sequences(scaled_values, seq_length)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float().reshape(-1, 1)\n",
    "X_val = torch.from_numpy(X_val).float()\n",
    "y_val = torch.from_numpy(y_val).float().reshape(-1, 1)\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float().reshape(-1, 1)\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, _ = self.lstm(input_seq)\n",
    "        predictions = self.linear(lstm_out[:, -1, :])\n",
    "        return predictions\n",
    "\n",
    "\n",
    "input_size = X_train.shape[2]\n",
    "model = LSTM(input_size=input_size, hidden_layer_size=100, num_layers=2, output_size=1).to(device)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_function, optimizer, epochs=150, patience=10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = np.inf\n",
    "    counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for seq, labels in train_loader:\n",
    "            seq, labels = seq.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(seq)\n",
    "            single_loss = loss_function(y_pred, labels)\n",
    "            single_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += single_loss.item() * seq.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for seq, labels in val_loader:\n",
    "                seq, labels = seq.to(device), labels.to(device)\n",
    "                y_pred = model(seq)\n",
    "                single_loss = loss_function(y_pred, labels)\n",
    "                val_loss += single_loss.item() * seq.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch} train loss: {train_loss}, val loss: {val_loss}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader, loss_function, optimizer, epochs=150,\n",
    "                                       patience=10)\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq, labels in test_loader:\n",
    "        seq, labels = seq.to(device), labels.to(device)\n",
    "        y_pred = model(seq)\n",
    "        test_predictions.append(y_pred.cpu().numpy())\n",
    "\n",
    "test_predictions = np.concatenate(test_predictions, axis=0)\n",
    "\n",
    "test_predictions = scaler.inverse_transform(test_predictions.reshape(-1, 1))\n",
    "y_test = scaler.inverse_transform(y_test.cpu().numpy().reshape(-1, 1))\n",
    "\n",
    "mae = np.mean(np.abs(test_predictions - y_test))\n",
    "mse = np.mean((test_predictions - y_test) ** 2)\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Plotagem das perdas de treinamento e validação\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "cd670cddfc773300",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Data e Hora  PM2.5\n",
      "0  2009-01-01 01:00:00    NaN\n",
      "1  2009-01-01 02:00:00    NaN\n",
      "2  2009-01-01 03:00:00    NaN\n",
      "3  2009-01-01 04:00:00    NaN\n",
      "4  2009-01-01 05:00:00    NaN\n",
      "-------------------\n",
      "               Data e Hora  PM2.5\n",
      "59962  2015-11-04 10:30:00   25.0\n",
      "59963  2015-11-04 11:30:00   13.0\n",
      "59964  2015-11-04 12:30:00   12.0\n",
      "59965  2015-11-04 13:30:00   14.0\n",
      "59966  2015-11-04 14:30:00   12.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 19\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(df\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m5\u001B[39m))\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Convertendo a coluna de data e hora para datetime\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mData e Hora\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241m.\u001B[39mto_datetime(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mData e Hora\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     20\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhour\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mData e Hora\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39mhour\n\u001B[0;32m     21\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mminute\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mData e Hora\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39mminute\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:937\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:928\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_311_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_311_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\PyCharm Professional 3\\plugins\\python\\helpers\\pydev\\pydevd.py:1201\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1198\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1200\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1201\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\PyCharm Professional 3\\plugins\\python\\helpers\\pydev\\pydevd.py:1216\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1213\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1215\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1216\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1218\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1220\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T20:33:46.035463Z",
     "start_time": "2024-07-04T20:33:45.068491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Carregar dados\n",
    "df = pd.read_csv(\n",
    "    './dados_tratados/combinado/Piratininga/Piratininga_tratado_combinado.csv',\n",
    "    usecols=['PM2.5', 'PM10', 'Dióxido de Enxofre', 'Monóxido de Carbono', 'Monóxido de Nitrogênio', 'Data e Hora'],\n",
    "    parse_dates=['Data e Hora'],\n",
    "    dtype={'PM2.5': str, 'PM10': str, 'Dióxido de Enxofre': str,\n",
    "           'Monóxido de Carbono': str, 'Monóxido de Nitrogênio': str})\n",
    "\n",
    "cols = ['PM2.5', 'PM10', 'Dióxido de Enxofre', 'Monóxido de Carbono', 'Monóxido de Nitrogênio']\n",
    "df[cols] = df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "df = df['PM2.5'].dropna()\n",
    "\n",
    "\n",
    "# Convertendo a coluna de data e hora para datetime\n",
    "df['Data e Hora'] = pd.to_datetime(df['Data e Hora'])\n",
    "df = df.set_index('Data e Hora')\n",
    "\n",
    "features = ['PM2.5', 'PM10', 'Dióxido de Enxofre', 'Monóxido de Carbono', 'Monóxido de Nitrogênio']\n",
    "data_values = df[features].values\n",
    "\n",
    "# Normalizando os dados\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_values = scaler.fit_transform(data_values)\n",
    "\n",
    "\n",
    "def create_sequences(data, window_size):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data) - window_size - 1):\n",
    "        x = data[i:(i + window_size)]\n",
    "        y = data[i + window_size][0]  # Prevendo o valor de PM2.5\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "\n",
    "seq_length = 8\n",
    "X, y = create_sequences(scaled_values, seq_length)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float().reshape(-1, 1)\n",
    "X_val = torch.from_numpy(X_val).float()\n",
    "y_val = torch.from_numpy(y_val).float().reshape(-1, 1)\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float().reshape(-1, 1)\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, _ = self.lstm(input_seq)\n",
    "        predictions = self.linear(lstm_out[:, -1, :])\n",
    "        return predictions\n",
    "\n",
    "\n",
    "input_size = X_train.shape[2]\n",
    "model = LSTM(input_size=input_size, hidden_layer_size=100, num_layers=2, output_size=1).to(device)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_function, optimizer, epochs=150, patience=10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = np.inf\n",
    "    counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for seq, labels in train_loader:\n",
    "            seq, labels = seq.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(seq)\n",
    "            single_loss = loss_function(y_pred, labels)\n",
    "            single_loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += single_loss.item() * seq.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for seq, labels in val_loader:\n",
    "                seq, labels = seq.to(device), labels.to(device)\n",
    "                y_pred = model(seq)\n",
    "                single_loss = loss_function(y_pred, labels)\n",
    "                val_loss += single_loss.item() * seq.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch} train loss: {train_loss}, val loss: {val_loss}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader, loss_function, optimizer, epochs=150,\n",
    "                                       patience=10)\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq, labels in test_loader:\n",
    "        seq, labels = seq.to(device), labels.to(device)\n",
    "        y_pred = model(seq)\n",
    "        test_predictions.append(y_pred.cpu().numpy())\n",
    "\n",
    "test_predictions = np.concatenate(test_predictions, axis=0)\n",
    "\n",
    "# Inversão da normalização apenas para o PM2.5\n",
    "test_predictions_pm25 = scaler.inverse_transform(\n",
    "    np.concatenate([test_predictions, np.zeros((test_predictions.shape[0], scaled_values.shape[1] - 1))], axis=1))[:, 0]\n",
    "y_test_pm25 = scaler.inverse_transform(\n",
    "    np.concatenate([y_test.cpu().numpy(), np.zeros((y_test.shape[0], scaled_values.shape[1] - 1))], axis=1))[:, 0]\n",
    "\n",
    "mae = np.mean(np.abs(test_predictions_pm25 - y_test_pm25))\n",
    "mse = np.mean((test_predictions_pm25 - y_test_pm25) ** 2)\n",
    "print(f'Mean Absolute Error: {mae}')\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Plotagem das perdas de treinamento e validação\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "dbcac0cd89a6a9df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Data e Hora'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32mC:\\dev\\scripts\\plots\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mindex.pyx:175\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\\\_libs\\\\index_class_helper.pxi:70\u001B[0m, in \u001B[0;36mpandas._libs.index.Int64Engine._check_type\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'Data e Hora'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 27\u001B[0m\n\u001B[0;32m     23\u001B[0m data \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPM2.5\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mdropna()\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# Convertendo a coluna de data e hora para datetime\u001B[39;00m\n\u001B[1;32m---> 27\u001B[0m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mData e Hora\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mData e Hora\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[0;32m     28\u001B[0m data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mset_index(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mData e Hora\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     30\u001B[0m features \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPM2.5\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPM10\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDióxido de Enxofre\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMonóxido de Carbono\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMonóxido de Nitrogênio\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32mC:\\dev\\scripts\\plots\\venv\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001B[0m, in \u001B[0;36mSeries.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   1118\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[key]\n\u001B[0;32m   1120\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m key_is_scalar:\n\u001B[1;32m-> 1121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1123\u001B[0m \u001B[38;5;66;03m# Convert generator to list before going through hashable part\u001B[39;00m\n\u001B[0;32m   1124\u001B[0m \u001B[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001B[39;00m\n\u001B[0;32m   1125\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n",
      "File \u001B[1;32mC:\\dev\\scripts\\plots\\venv\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001B[0m, in \u001B[0;36mSeries._get_value\u001B[1;34m(self, label, takeable)\u001B[0m\n\u001B[0;32m   1234\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[label]\n\u001B[0;32m   1236\u001B[0m \u001B[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001B[39;00m\n\u001B[1;32m-> 1237\u001B[0m loc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1239\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(loc):\n\u001B[0;32m   1240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[loc]\n",
      "File \u001B[1;32mC:\\dev\\scripts\\plots\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[0;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[0;32m   3810\u001B[0m     ):\n\u001B[0;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[1;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'Data e Hora'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cb13c935ae3d1a65",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
